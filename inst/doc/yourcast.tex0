\documentclass[oneside,letterpaper,titlepage]{article}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{natbib} 
\usepackage[reqno]{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{epsf}
\usepackage{url}
\usepackage{html}
\usepackage{dcolumn}
\usepackage{longtable}
\usepackage{vmargin}
\topmargin=0in

%\setpapersize{USletter}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
%\pagestyle{myheadings}
\htmladdtonavigation{
  \htmladdnormallink{%
    \htmladdimg{http://gking.harvard.edu/pics/home.gif}}
  {http://gking.harvard.edu/}}
\newcommand{\hlink}{\htmladdnormallink}

\bodytext{ BACKGROUND="http://gking.harvard.edu/pics/temple.jpg"}
\setcounter{tocdepth}{3}

\newcommand{\YourCast}{\textsc{YourCast}}

\title{\YourCast: Software for Simultaneous Time Series Forecasting
  with Your Assumptions\thanks{Available from
    http://GKing.Harvard.Edu/yourcast.}}

\author{Federico Girosi\thanks{Policy Researcher, RAND Corporation,
    (1700 Main Street, PO Box 2138, Santa Monica, CA 90407-2138;
    \texttt{http://www.ai.mit.edu/people/girosi/},
    \texttt{girosi@rand.org}, (310) 393-0411 x7794)}
\and %
Gary King\thanks{David Florence Professor of Government, Harvard
  University (Center for Basic Research in the Social Sciences, 34
  Kirkland Street, Harvard University, Cambridge MA 02138;
  \texttt{http://GKing.Harvard.Edu}, \texttt{King@Harvard.Edu}, (617)
  495-2027).}}
%
\date{\today\\
  ~\\
  Lead Programmer: Elena Villal\'{o}n\\
  Graphics Programming: Nirmala Ravishankar\\
  Documentation: Heather Stoll}

\makeindex

\begin{document}\maketitle


\begin{rawhtml} 
  <p> [Also available is a down-loadable <a
  href="/yourcast/docs/yourcast.pdf">PDF</a> version of this entire
  document]
\end{rawhtml}

\tableofcontents
\clearpage

\section{Introduction}

\YourCast\ implements the methods for demographic forecasting
discussed in
\begin{quote}
  Federico Girosi and Gary King. 2004.  \emph{Demographic
    Forecasting}.  Book manuscript in progress available from
  \url{http://gking.harvard.edu/}.
\end{quote}
Please read at least Chapter 1 from this manuscript before attempting
to use \YourCast.

At its most basic, \YourCast\ runs linear regressions, and estimates
the usual quantities of interest, such as forecasts, causal effects,
etc.  The benefit of running \YourCast\ over standard linear
regression software comes from the improved performance due to
estimating sets of regressions together in sophisticated ways.

\YourCast\ avoids the bias that results from stacking datasets from
separate cross-sections and assuming constant parameters, and the
inefficiency that results from running independent regressions in each
cross-section.  \YourCast\ instead allows you to tie the different
regressions together probabilistically in ways consistent with what
you know about the world and your data.  The model does not require
that you have the same covariates with the same meaning measured in
every cross-section.

For example, one might assume that the separate time series
regressions in neighboring (or ``similar'') countries are more alike.
Our approach is fully Bayesian, but you need not assume that
\emph{coefficients} (which are never observed) in neighboring
countries are similar.  \YourCast\ makes it possible to assume instead
that neighboring countries are similar in their values or trends in
the \emph{expected value of the dependent variable}.  This approach is
advantageous because prior knowledge almost always exists about the
dependent variable, and the expected value is always on the same
metric even when including explanatory variables that differ in number
or meaning in each country.

The power of \YourCast\ to improve forecasts comes from allowing one
to smooth in many sophisticated ways, in addition to across countries.
You can thus decide whether to smooth over indices that are
geographic, grouped versions of underlying continuous variables (such
as age groups), time, or interactions among these.  For example, you
can assume that, unless contradicted by the data, forecasts should be
relatively smooth over time, or that the forecast time trends should
be similar in adjacent age groups, or even that the differences in
time trends between adjacent age groups stay roughly similar as they
vary over countries.  The model works with time-series-cross-sectional
(TS-CS) data but also data for which the time series varies over more
than one cross-section (TS-CS-CS-CS\ldots\ data such as log-mortality
over time by age, country, sex, and cause).  The specific notion of
``smoothness'' or ``similarity'' used in \YourCast\ is also your
choice.  The assumptions made by the statistical model are therefore
governed your choices, and the sophistication of those assumptions and
the degree to which they match empirical reality are, for the most
part, limited only by what you may know or are willing to assume
rather than arbitrary choices embedded in a mathematical model.  In
our work, we have found that \YourCast\ makes it possible to improve
forecasts well beyond that possible with traditional regression (or
autoregression) strategies.

\section{Installation}

\YourCast\ requires \texttt{R} version 1.71 or later and the package
\texttt{sma}, Functions for Exploratory Micro-array Analysis, both
available from \url{http://cran.r-project.org/}.  Install \texttt{sma}
if you have not already done so by typing
  \begin{verbatim}
  >install.packages("sma") .
  \end{verbatim}
Installation of \YourCast\ differs slightly by operating system.

\subsection{Linux/Unix}

From the R command line, type
  \begin{verbatim}
  > install.packages("YourCast", CRAN="http://gking.harvard.edu", destdir="~/") .
  \end{verbatim}
Alternatively, from a Linux/Unix shell, download the Unix bundle
\texttt{YourCast\_1.0.tar.gz}, available from
\url{http://gking.harvard.edu}.  Then, in the same directory as the
downloaded file, at the Linux/Unix command line, type
  \begin{verbatim}
  > R CMD INSTALL --library=.R/library ~/YourCast_1.0.tar.gz  
  \end{verbatim}

\subsection{Windows}

From the \texttt{R} command line, type
  \begin{verbatim}
  > install.packages("YourCast", CRAN="http://gking.harvard.edu") .
  \end{verbatim}
Alternatively, download the Windows bundle from
\url{http://gking.harvard.edu/yourcast}.  From the GUI, click on the
menu ``Packages'', click on the option ``Install package(s) from local
zip files'', and select the zip file that you downloaded.

\section{User's Guide}

The \YourCast\ library contains two functions: \texttt{yourcast()},
which performs data preprocessing and forecasting; and
\texttt{yourgraph()}, which produces graphical summaries of the
forecasts.

Data processing prior to estimating the forecasting model takes
sufficient time with large datasets that we make it possible to save
time by reusing preprocessed data from a prior run.  The function
\texttt{yourcast()} accordingly:
\begin{enumerate}
\item Sets the options you supply either in a file (indicated by
  the value of the argument \texttt{userfile})
  or as arguments to \texttt{yourcast()}.  If both
  are used, the arguments to the function take precedence.
  
\item Either processes the input data files containing explanatory
  and dependent variables or loads a preprocessed
  data file computed during a previous run (as indicated by
  the value of the argument \texttt{reuse.data}).
  
\item Forecasts the dependent variable using the
  method you select via the argument \texttt{model}.  The forecast
  object is either returned, saved in an output file, or both,
  depending on the value of the argument \texttt{save.output}.  The 
  simulation creates, by default, an output directory within your 
  working directory, if none exists. 
\end{enumerate}
The function \texttt{yourgraph()} takes as its input either the
forecast object generated by \texttt{yourcast()} or a
similarly-formatted file.  For a given level of the non-geographic
nominal classificatory variable such as gender, it generates two
summary graphs by default for each level of the geographic nominal
classificatory variable such as country.  The first graph 
is a time series plot of both the observed and forecasted
values of the dependent variable for each level of the grouped
continuous variable such as age.  The second graph plots both the
observed and forecasted values of the dependent variable against the
levels of the grouped continuous variable such as age for each year.

\subsection{Introduction by Examples}

We now introduce \YourCast\ by via three increasingly sophisticated
examples.  All three examples use the same data, which we describe
first.

\subsubsection{Data Input}

Included with the \YourCast\ package are data from \citet{GirKin04} on
all-cause mortality and two covariates, tobacco consumption and per
capita gross domestic product (GDP), for ten countries and two
genders.  The goal is to generate forecasts of mortality in each
country-cause-age-sex group.

The input data structure thus is an annual time-series
cross-classified by a two-way cross-section of country by age group.
The data is further cross-classified by gender.  Forecasts will be
constructed for one of the two genders by initially sub-setting the
data.  ``Country'' is a geographic nominal variable that has ten
levels in this data set (the codes representing each level are shown
in parentheses), ``U.K.''  (4308), ``U.S.''  (2450), ``Brazil''
(2070), ``Colombia'' (2130), ``Canada'' (2090), ``Mexico'' (2310),
``Ireland'' (4170), ``France'' (4080), ``Venezuela'' (2470), and
``Germany'' (4085).  ``Gender'' is a non-geographic nominal variable
with two levels (codes again shown in parentheses), ``males'' (2) and
``females'' (3). ``Age'' is a grouped continuous variable with 17
levels, age groups ranging from 0--4 years (represented by the code
00) to 80+ years (represented by the code 80).  ``Year'' ranges from
1920 to 2000 for covariates and from 1950 to 2000 for mortality and is
represented by the obvious four-digit code, although the length of the
time series varies across countries.  Each country--age group--year
unit is represented by an identification code in the data files, which
combines the codes representing the country, age group, and year in
that order.  For example, the identification code for Brazil, ages
0--4 years, in 1950 is 2070001950.  Missing data is identified with 
either the number ``$-999.0000$'' or the string ``NA''.

The dependent variable data file contains three columns and is named
\texttt{allc.txt}, which stands for all causes of mortality.  The first
column contains the value of all-cause mortality for the country--age
group--year unit, the second the identification code for that
unit, and the third the code for the gender.  It looks as follows:
\begin{verbatim} 
        -999.0000 2070001950 2
        -999.0000 2070001950 3
        -999.0000 2070001951 2
        -999.0000 2070001951 3
        ...
        0.009867 2070002000 2
        0.007639 2070002000 3
        ...
        -999.0000 2070801950 2
        -999.0000 2070801950 3
        ...
        0.2098 2070802000 2
        0.1701 2070802000 3
        ...
        0.007861 4308001950 2
        0.006149 4308001950 3
        ...
        0.0009882 4308301980 2
        0.0006124 4308301980 3
        ...
        0.1488 4308802000 2
        0.1182 4308802000 3
\end{verbatim}
As you can see from the file, each country-age-year unit has two
entries, one for each gender, since all-cause mortality varies with
gender.

The structure of the covariate data files may differ from that of the
dependent data file, depending on over which units the covariates vary.
Data on the covariate GDP varies with the country and year but
not age group or gender.  Accordingly, the data file for this
covariate only has two columns, the first containing the
value of GDP for the country--age group--year unit and the second
the identification code for that unit. This file is shown below:
\begin{verbatim}
        1039.767 2070001920
        1054.972 2070001921
        ...
        6993.039 2070002000
        ...
        1047.840 4308001920
        ...
        7103.992 4308002000
\end{verbatim}
Only one age group, here ages 0--4 (represented by the code 00), appears 
in the file and each country--year unit is featured once.  For this
type of covariate, the value of the covariate for a country--year will
be automatically replicated for the other age groups.  The value of 
the covariate tobacco varies with the country, age group, 
gender, and year.  Hence, unlike the GDP covariate, its data file looks like 
the mortality data file as shown below:  
\begin{verbatim}
        0 2070001920 2
        0 2070001920 3
        0 2070001921 2
        0 2070001921 3
        ...
        0 2070002000 2
        0 2070002000 3
        ...
        1082.922 2070801950 2
        57.078 2070801950 3
        ...
        1068.903 2070802000 2
        188.097 2070802000 3
        ...
        0 4308001920 2
        0 4308001920 3
        ...
        1712.081 4308301980 2
        952.919 4308301980 3
        ...
        1107.765 4308802000 2
        604.235 4308802000 3
\end{verbatim}
The only difference is that the value of tobacco consumption is
substituted for the value of all-cause mortality in the first column.

\subsubsection{Example 1}

We are now ready to proceed with forecasting.  The first step is to
create a sub-directory of our \texttt{R} working directory, which we
will call ``DATA/''.  We place the data files for our covariates
(\texttt{tobacco.txt} and \texttt{gdp.txt}) and dependent variable
(\texttt{allc.txt}) in this directory.  We next create the directory
``PRIOR/'' in which we place the adjacency matrix used to smooth over
countries (\texttt{adjacency.txt}).  Optionally, we might also put the
adjacency file in the general directory ``DATA/''.  We run \texttt{R}
and load the \YourCast\ package by typing
\begin{verbatim}
     > library(YourCast)
\end{verbatim}
at the command prompt.  The main function of the simulation is
\texttt{yourcast()}, and its output is the preprocessed data file and
the forecast object, both of which can be stored in output files on
disk and the latter of which is also returned as a list.  The function
\texttt{yourcast()} has some default arguments that you may change as
part of your choices for forecasting.  The default arguments are
stored in an internal \texttt{R} file.  They can be reviewed by typing
\begin{verbatim}
     > args.default()
\end{verbatim}
at the command prompt, which lists all of the arguments' names in the 
\texttt{yourcast()} function as well as their default values.   

Moving on to the actual forecasting, the first task is to decide how
our forecasts will be constructed.  We initially decide to follow
conventional demographic practices and produce forecasts for a
transformed dependent variable, log all-cause mortality.  For this
first attempt at forecasting, we decide to include all ten countries
and 17 age groups in the analysis.  We also decide to forecast log
mortality for men; to include as covariates in our model tobacco
consumption, GDP, and a constant; and to construct forecasts for the
period 2001--2030.  We next need to select a forecasting model.  We
choose to use the Girosi and King method, which requires us to make
several additional decisions.  We choose to smooth the expected value
of log mortality over countries, as we believe that mortality rates
should be similar for neighboring countries.  We also decide to use a
zero-mean prior.  Finally, we choose to implement the method without
Gibbs sampling, that is to use the Bayes Maximum A Posteriori
technique because of the latter's relatively greater processing speed.

These decisions are communicated to \texttt{yourcast()}, the function
that processes the input data files and constructs forecasts, by
typing
\begin{verbatim}
     > my.forecast <- yourcast(model = ``MAP'', Hct.c.deriv = 
     + TRUE, Ha.sigma = NA, Ht.sigma = NA, Hat.sigma = NA,  
     + zero.mean = TRUE, depvar = ``allc'', strata = 2,  
     + cov = c(``cnst'',``time'', ``tobacco'', ``gdp''), 
     + cov.type = list(tobacco = NA, gdp = 
     + ``strata.age.independent''))
\end{verbatim}
at the command prompt.  \texttt{yourcast()} assigns to the output
object a list containing the actual and predicted values of log
all-cause mortality along with all the estimated model parameters and
options just chosen.  The names of the parameters and statistics in
this list, which we chose to call \texttt{my.forecast}, may be
obtained by typing
\begin{verbatim}
     > names(my.forecast)
\end{verbatim}
at the command prompt.  The predicted out-of-sample values of
all-cause log mortality can be accessed by typing
\begin{verbatim}
     > my.forecast$yhatout
\end{verbatim} %$
at the command prompt.  Note that this is itself a list: each element
is a matrix that contains the out-of-sample forecasts for a country,
where the rows of the matrix represent years and the columns age
groups.  To access the out-of-sample predictions for the United
States, for example, type
\begin{verbatim}
     > my.forecast$yhatout$``2450''
\end{verbatim}
at the command prompt.  Similarly, we can view the predicted in-sample
values of all-cause log mortality by typing
\begin{verbatim}
     > my.forecast$yhatin
\end{verbatim} %$
and the estimated coefficients by typing
\begin{verbatim}
     > my.forecast$coeff
\end{verbatim} %$
at the command prompt.  Each element of the latter list is a vector of
estimated coefficients for a country--age group.

Keep in mind that a processed data file containing the various
processed data objects that are used in forecasting is saved to the
sub-directory ``OUTPUT/'' of the working directory, which is created
by \texttt{yourcast} if it does not already exist.  The name of this
file is constructed from the inputs: in this case it is
\texttt{data\_allc\_2.dat}.  The simulation always generates the
preprocessed data file, except when the parameters of the simulation
are the same as those of a previous run and the preprocessed data can
be read from an existing file.  The preprocessed file can be deleted
at any time without loss of information, but keeping it on the disk
can save processing time for the next run.  A final point about the
outputs of \texttt{yourcast} is that setting the input parameter
\texttt{save.output= T} saves an output file that contains the
dependent variable and the forecasting results to the sub-directory
``OUTPUT/'' of the working directory.  The name of the file is
constructed from the inputs, as before, and
is \texttt{allc\_2\_MAP.dat} for this example.
%
%GK: do we want the results in a file as well as the output object?
% let's get rid of the repetition.
%
%HMS:  If you want to do so, by all means.  Some things are saved
%in the output file that are not returned, but this could easily
%be changed (or may have already been, in the course of all of the
%recent revisions).  Will you check with Federico to see if he agrees?
%
% EV: You need the preprocessing file \texttt{data\_allcause\_2.dat} 
% to re-use it in further calculations, 
% because you may not want to change the input parameters but the model. 
% I could define a new global such as save.preprocessed, 
%to turn down the option of saving the preprocessing stuff, 
%if you would like me to. We always felt that it is more useful 
%to save it than not and that the option will be of little help.  
% Preprocessing file contains variables that are not saved in the output 
% object such as insampx (covariates data) and that 
%you may want to plot or have a look at.
% The output file have the estimate yhat's for the model and it is the 
% object return by yourcast as a list, and also save in the output file 
%\texttt{allcause\_2\_MAP.dat}. 
% You have the choice of not saving the output into a file 
%but just work with the return object by setting save.output = F; 
%the save.output is an additional features that may be handy 
%but you have the choice of not using. Nirmala programs are 
%reading the data either from the output object or from the output 
%file. There is some repeating between the two files for insampy and outsampy, 
%but that is all, because is needed for the graphics and Nirmala is not using
% the preprocessing file at all; the rest of repetitions are 
% very small parameters, which you do not really care 
%to repeat such as the list of countries and model name. 
%Note also that the preprocessing file contains names of the old code 
%convention of names, such as whoinsampy, we have one function 
%that helps to read the preprocessing file into the names we like insampy. 
%However, this will get into the way of the graphics programs and 
%make things unnecessarily complicated, better just the output file 
%with everything the graphics need or the object. end of EV

Let's now review how we communicated our choices about constructing
the forecasts to \texttt{yourcast}.  By supplying the value
\texttt{``MAP''} to the parameter \texttt{model}, we told
\texttt{yourcast()} that we wanted to use the Girosi-King forecasting
method that is implemented by the Bayes Maximum A Posteriori
technique.  Setting the parameter \texttt{Hct.c.deriv} equal to TRUE
told \texttt{yourcast()} to smooth over countries (and setting the
parameters \texttt{Ha.sigma}, \texttt{Ht.sigma}, and
\texttt{Hat.sigma} to NA told it to \emph{not} smooth over age groups,
time, and the time trend over age groups, respectively).  Setting the
parameter \texttt{zero.mean} to TRUE told \texttt{yourcast()} to use a
zero mean prior.  Setting \texttt{depvar} to \texttt{``allc''}
identified the dependent variable as all-cause mortality as well as
told \texttt{yourcast()} that the input dependent variable data file
was named \texttt{allc.txt}.  The string vector to which we set the
parameter \texttt{cov} tells \texttt{yourcast()} to include four
covariates in the model---a constant, time, tobacco consumption, and
GDP--- as well as that the latter two covariate data files are named
\texttt{tobacco.txt} and \texttt{gdp.txt}, respectively.  The value of
the parameter \texttt{cov.type} told \texttt{yourcast()} that tobacco
is both gender and age dependent (meaning that it varies over gender
and age) while GDP is independent of (i.e., constant over) both. The
value of the parameter \texttt{strata} told \texttt{yourcast()} to
construct forecasts for men, since the code for the gender ``male'' is
``2''.
 
Parameters not explicitly listed are set to their default values.  For
example, \texttt{transform} has default value \texttt{1}, which tells
\texttt{yourcast()} to log the dependent variable.  The parameter
\texttt{data.path} indicates where to look for the data files and has
default value ``DATA/''; \texttt{prior.path} identifies the
location of the pre-computed quantities related to the prior and also
has default value ``DATA/''.  The parameter
\texttt{Hct.cntry.weight} passes information about how to weight each
country when smoothing; we weighted each country equally by relying on
the default value of 0.  Similarly, by relying on the default value of
\texttt{Hct.time.weight}, we weighted each year equally.  The
parameter \texttt{lag} tells \texttt{yourcast()} how many periods to
lag the covariates.  Relatedly, \texttt{fore} identifies the last year
for which we want to construct forecasts and \texttt{yrest} the last
year for which we have data for at least some units on the dependent
variable and covariates.  The default value of \texttt{fore},
\texttt{2030}, produces forecasts through 2030 and the default value
of \texttt{lag} is \texttt{30}.  Choosing a 30-year lag with our
one-step-ahead forecasting method is one way to forecast 30 years into
the future without using any information beyond the last year of the
data (which is 2000 in the example).  Other parameters tell
\texttt{yourcast()} how to read the cross-sectional time series
identification code.  Finally, the default value of the parameter
\texttt{save.output} is TRUE, which instructs \texttt{yourcast()} to
save an output file to the subdirectory ``OUTPUT/'', the default
value of the parameter \texttt{out.path}.

\subsubsection{Example 2}

Working with the same data set, we now modify some of the decisions
that we initially made above.  Instead of smoothing only over countries, 
we now smooth the level of the expected value of log mortality over
\emph{both} countries and age groups.  We also decide to impose
greater smoothness on older age groups than younger ones, i.e. to
penalize older age groups more for lack of smoothness than younger
ones, since there are well-known sharp changes in log-mortality around
age five but not for older ages.  Specifically, we decide to penalize
proportional to the level of the group.  We include the
square of GDP as a covariate in addition to the four covariates used
during the first round of forecasting.

As before, we implement these decisions by typing the following
\begin{verbatim}
     > my.forecast <- yourcast(model = ``MAP'', Hct.c.deriv = 
     + TRUE, Ht.sigma = NA, Hat.sigma = NA, Ha.age.weight = 1
     + zero.mean = TRUE, depvar = ``allc'', strata = 2, 
     + cov = c(``cnst'', ``time'', ``tobacco'', ``(gdp)2'',  
     + ``allc''), cov.type = list(tobacco = NA, gdp = 
     + ``strata.age.independent''))
\end{verbatim}
at the command prompt.  By no longer setting the parameter
\texttt{Ha.sigma} equal to NA, we rely upon its default value, which
instructs \texttt{yourcast()} to smooth over age groups.  This default
value represents our uncertainty about the prior, which we leave as is
for the time being since we are reasonably confident that log
mortality should be smooth across adjacent age groups.  By setting the
parameter \texttt{Ha.age.weight} equal to 1, we weight the age groups
proportional to their level.  We could increase the penalty applied to
older age groups by using a larger scalar such as 2.  By relying upon
the default value of the parameter \texttt{Ha.deriv}, we use the
default smoothness functional when smoothing over age groups (a
standard functional that penalizes the second derivative).
Additionally, by relying upon the default value of the parameter
\texttt{Ha.time.weight}, we weight each year equally when smoothing
over age groups.  We also add the square of GDP to the covariates by
replacing the original string \texttt{``gdp''} with the string
\texttt{``(gdp)2''} in the vector of covariates. Note that the program
includes \texttt{``gdp''} automatically if the second order term is
specified. Thus, you only need to include the highest power in a
series of powers of any covariate in the covariate string.  We also
include the dependent variable, \texttt{``allc''}, in the covariate
vector.  Be aware that if the dependent variable is included in the
vector of covariates, you may loose a large portion of your data after
lagging the covariates due to missing values.

The name of the output file containing the forecast object is
\texttt{allc\_2\_MAP.dat} for this example, as before.

\subsubsection{Example 3}

Now suppose that we want to forecast mortality from cardiovascular
disease (not log all-cause mortality), but only for European
countries.  We're also not certain that our time series are long
enough to justify forecasting thirty years into the future, so we
decide to only forecast mortality through 2015.  We consequently
decide to set the lag to 15 to make good use of our data.  We
additionally decide to smooth the level of expected log mortality over
time and to smooth the time trend of the expected value over age
groups.  Specifically, we weight all years and age groups equally when
smoothing over time and use a mixed smoothness functional that
penalizes the first and second derivative equally (separately
controlling the form of prior indifference and degree of smoothness,
respectively).  Also, we choose to use the natural log to the third
power of the covariate tobacco.  Further, although we think that the
time trend is smooth over age groups, we do not feel confident about
this decision.  Finally, we decide that we want to implement the
Girosi-King method using Gibbs sampling.

This time around, we type
\begin{verbatim}
     > my.forecast <- yourcast(model = ``BAYES'', Hct.c.deriv + =
     + TRUE, Ht.deriv = c(0, 1, 1), Ha.age.weight = 1, + Hat.sigma =
     + 0.9, zero.mean = TRUE, depvar = ``cvds'', + strata = 2, cov =
     + c(``cnst'', ``time'', ``ln(tobacco)3'', + ``(gdp)2'',
     + ``(allc)2'', ``cvds'', ``ln(cvds)2''), + cov.type =
     + list(tobacco = NA, gdp = + ``strata.age.independent'',
     + allc=``depvar.like''), + usercntrylist = c(4308, 4085, 4080,
     + 4170), transform = 0, + fore = 2015, lag = 15)
\end{verbatim}
at the command prompt.  We restrict the analysis to a subset of the
available countries by supplying a numeric vector for the parameter
\texttt{usercntrylist}: each element of this vector is the code
corresponding to a country to include in the analysis, here the codes
for the U.K., Germany, France, and Ireland.  The dependent variable is
now cardiovascular disease, which we tell \texttt{yourcast} by setting
the parameter \texttt{depvar} equal to \texttt{``cvds''}. We forecast
through 2015 with the desired lag by setting the parameter
\texttt{fore} equal to 2015 and \texttt{lag} equal to 15.  We told
\texttt{yourcast} to use the third power of the natural log of the
covariate tobacco by modifying the string ``tobacco'' to read
``ln(tobacco)3''.  (Recall that the first power of GDP is included
automatically after specifying the second).  We set
\texttt{``transform''} equal to 0, so that the output object will
contain the dependent variable data and the model predictions as the
input data without any transformation.  Note that if we include the
natural log of the dependent variable as a covariate, the parameter
\texttt{``transform''} must not be set to 1 or 3. The covariate
\texttt{allc} is identified as a potential dependent variable by
setting the parameter \texttt{cov.type} equal to ``depvar.like'';
however, the covariate \texttt{cvds} is automatically recognized by
the simulation to be of that type because it is elsewhere identified
as the dependent variable (by the parameter ``depvar'').

With respect to the forecasting model itself, we told
\texttt{yourcast} to implement the Girosi-King method with Gibbs
sampling by setting the parameter \texttt{model} equal to the string
\texttt{``BAYES''}.  We smooth over time and the time trend over age
groups by no longer setting the parameters Ht.sigma and Hat.sigma
equal to NA, respectively.  Further, when smoothing over time, we use
the mixed smoothness functional desired by setting the parameter
\texttt{Ht.deriv} equal to c(0, 1, 1); the elements of this vector are
the weights on the 0th, 1st, and 2nd order derivatives, respectively,
with equal weights on the latter two signaled by assigning to both
weights equal to 1.  By relying on the default values of the
parameters \texttt{Ht.age.weight} and \texttt{Ht.time.weight}, we
weight both all age groups and years equally when smoothing over time.
We express our uncertainty about the knowledge embodied in the prior
from smoothing the time trend over age groups by setting the parameter
\texttt{Hat.sigma} equal to a larger scalar than the default value of
0.2, which allows the data to play a greater role in model estimation.
Note that by relying on the default values of the parameters
\texttt{Hat.a.deriv}, \texttt{Hat.t.deriv}, \texttt{Hat.age.weight},
and \texttt{Hat.time.weight}, we weight all age groups and years
equally when smoothing the time trend over age groups and retain the
default components of the smoothness functional that pertain to this
interaction.

Our output file containing the forecast object is now named
\texttt{cvds\_2\_Bayes.dat}.

\subsection{Data Input}\label{sec:data}

\subsubsection{Data Structure}\label{sec:datastr}
The methods implemented by \YourCast\ apply to data with a particular
cross-section time series structure.  \YourCast\ requires
cross-sections to be defined over a grouped continuous variable such
as age and a nominal-level variable such as a country or other
geographic variable.  Cross-sections may further be defined over a
non-geographic nominal variable such as gender or other strata.  In
other words, the data must have a one-, two-, or three-way
cross-classified cross-section time series (hereafter, CSTS)
structure.  For example, the CSTS unit may be a
gender--country--year--age group such as British females of ages 20--25
in 1990 or a state--year--income group, such as individuals earning
between \$50,000 and \$75,000 in New York in 2000.

Note that the non-geographic nominal index variable is used to subset
the data so that all model-building and forecasting is done for only
one strata.  The simulation will not smooth over this variable.  In
the case of forecasting mortality, the non-geographic nominal index
variable is gender.  All model-building and forecasting is done for
one of two strata, either `men' or `women'.  Hence, the remainder of
this document uses the phrase `CSTS unit' to mean a two-way
cross-classified cross-sectional time-series unit, which may be for a
particular strata of a non-geographic nominal index variable.  For
forecasting mortality, this is a country-year-age group for either men
or women. Similarly, it uses the phrase `cross-sectional unit' or `CS
unit' to mean a two-way cross-classified cross-sectional unit, which
again may be for a particular strata of a non-geographic nominal index
variable.  For forecasting mortality, this is a country--age group for
either men or women.

\YourCast\ allows the geographic nominal and grouped continuous index
variables to have any (positive integer) number of levels.  For
example, the WHO mortality forecasting data sets have data on 191
levels of the geographic nominal index variable and 17 levels of the
grouped continuous index variable.

Data with a single cross-sectional index variable, which may be
cross-classified by a non-geographic nominal variable, can be supplied
to \YourCast\ as follows.  For a data structure defined solely by a
geographic nominal index variable, you should define a `fake' or
constant level of the grouped continuous index variable.  For example,
let `00' be the code for the fake group, with CSTS indices
such as the following:
\begin{verbatim}
        2450001950 
        2450001951 
        2450001952 
        ... 
        2090001950 
        2090001951 
        2090001952. 
\end{verbatim}
The analysis subsequently proceeds as described for two- or three-way
cross-classified cross-section time series data structures, with the
proviso that covariates should not be assigned either the type
``age.independent'' or ``strata.age.independent''.
For a data structure defined solely by a grouped continuous variable,
one can similarly define a `fake' level of the geographic nominal
variable.  For example, let `99' be the code for the fake level, with
CSTS indeces such as the following:
\begin{verbatim}
        99001950 
        99001951 
        99001952 
        ... 
        99051950 
        99051951 
        99051952
\end{verbatim}
and, as before, proceed as usual.  No restrictions on covariate types
exist with this data structure.

\subsubsection{Input Data Files}\label{sec:datafil}
The data files must be in the following ASCII formats with extensions
\texttt{.txt}.  Data files for forecasting mortality are available
from \url{http://gking.harvard.edu}.  You may alternatively or
additionally create and supply your own data files for other
forecasting applications.  Some of the data files for forecasting
mortality, e.g. the adjacency matrix for levels of the geographic
nominal variable, may be easily modified for use with other
applications.

Throughout, $n$ refers to the number of cases (CSTS units) and $m$ to
the number of selected covariates exclusive of time, a lag of the
dependent variable, a constant, and transformations of covariates.
Missing data must be denoted by $-999.000$ and/or \texttt{NA} 
in the data files. (\YourCast\ might eventually be modified to allow you
to choose your own representation for missing data.)  The unique identifying
code of each CSTS unit in the data files combines codes for the first
and second cross-sectional classifications with a code for the time
classification.  For example, for mortality forecasting, these are a
country code, an age group code, and a year code, such as 2450451985
for the U.S. (2450), age group 45-50 (45), and year 1985 (1985).

\paragraph{Covariates}
Covariate data files should be stored in the directory supplied as
argument \texttt{cov.path}.  Each covariate must be in a separate
file, which must have the same name as the covariate.  For example, if
a covariate was supplied to the argument \texttt{cov} as the string
\texttt{``tobacco''}, the name of the data file for this covariate
must be \texttt{tobacco.txt}.  Each line in the file must be
white-space delimited.  Covariate data files contain either two or
three columns: the first column is the numerical value of the covariate,
the second is the cross-section time series index code, and the
third, if it exists, is the code for the strata.  CSTS units may appear
in any order in the data file.  Although the examples below show the
cases organized first by cross-section time series index code and then
by strata, you are under no obligation to sort the data in this manner.  

Within each covariate type, as designated by \texttt{cov.type}, the
covariates must be specified, possibly with missing values, for all of
the cross-sections that apply to that type.  For example, if GDP (a
grouped continuous and strata independent covariate) is not available
for a specific level of the geographic nominal variable, the GDP data
for that level must appear as missing in the data file.

The number of columns in the covariate data files depend first 
upon whether or not there is a cross-classifying 
non-geographic nominal variable as specified by argument 
\texttt{strata} and second upon the type of the 
covariate as specified by the argument \texttt{cov.type}.

\medskip
\noindent \emph{Cross-Classifying Non-Geographic Nominal Variable.}
If there is a cross-classifying non-geographic nominal
variable, i.e., argument \texttt{strata} is not equal to NA, data file
formats vary as follows with the five possible covariate types.
\begin{enumerate}
\item If the covariate is declared strata and grouped continuous
  variable dependent, the default value of argument \texttt{cov.type}
  when there is a strata, it is stored in the three column format.  
  The file will contain a total of $n*l$ lines, where $l$ is the 
  number of levels of the non-geographic nominal index variable: each 
  CSTS unit will have $l$ lines, one for each level or strata.  For 
  example, when forecasting mortality, the covariate tobacco is gender 
  and age dependent.  The file might look as follows (with men coded as 
  2 and women coded as 3):
\begin{verbatim}
        365.25 2090151920 2
        59.75 2090151920 3
        343.765 2090151921 2
        56.235 2090151921 3
        ...
        558.835 2450151920 2
        91.165 2450151920 3
        642.231 2450151921 2
        93.97 2450151921 3
        ...
        544.756 2450451920 2
        105.244 2450451920 3
        561.517 2450451921 2
        108.483 2450451921 3
        ...
\end{verbatim}
  Note that each country-age-year group has two lines, one for each
  gender.
\item If the covariate is declared ``strata.independent'', it must be
  stored in the two-column format: the first column contains the value of
  the covariate and the second the index.  The file will contain a
  total of $n$ lines, one for each CSTS unit.  For example, when
  forecasting mortality, the covariate foo varies with age but not
  with gender.  The file might look as follows:
\begin{verbatim}
        5.2 2090001920
        6.4 2090001921
        6.3 2090001922
        ...
        25.7 2090051920
        29.3 2090051921
        29.9 2090051922
        ...
        4.5 2450001920 
        3.0 2450001921 
        3.1 2450001922 
        ...
        20.2 2450051920
        21.3 2450051921
        22.5 2450051922
        ...
\end{verbatim}
  The values of this covariate will be used by \texttt{yourcast()} for
  whichever strata is selected for analysis.
\item If the covariate is declared ``age.independent'', it must be
  stored in the three-column format.  However, the cross-sectional
  index runs over only one level of the grouped continuous variable
  (it does not matter which one is used).  The file will contain
  $c*t*l$ lines, where $c$ is the number of levels of the geographic
  nominal index variable, $t$ is the length of the time series, and
  $l$ is as before (if the length of the time series is identical for
  each level of the geographic nominal index variable): each
  geographic-time unit will have $l$ lines, one for each strata.  For
  example, when forecasting mortality, the covariate goo varies with
  gender but not with age.  The file might look as follows:
\begin{verbatim}
        12 2090001920 2
        64 2090001920 3
        16 2090001921 2
        73 2090001921 3
        ...
        94 2450001920 2
        30 2450001920 3
        101 2450001921 2
        20 2450001921 3
        ...
\end{verbatim}
  Note that each country-time unit has two lines, one for each gender,
  and that the index runs over the age group 00.  \texttt{Yourcast}
  will automatically replicate the values of this covariate for all
  other levels of the grouped continuous index variable.
\item The type ``strata.age.independent'' (or ``age.strata.independent'')
  is a combination of the previous two options.  For example, when 
  forecasting mortality, the covariate GDP does not vary over either 
  gender or age; it only varies across countries.  Accordingly, it should 
  be declared to be of this type.  The file might look as follows (with 
  GDP expressed per capita):
\begin{verbatim}
        4191.799 2090001920
        4410.895 2090001921
        4641.443 2090001922
        ...
        7024.985 2450001920
        7225.81 2450001921
        7432.376 2450001922 
        ...
\end{verbatim}
  Note that the country component of the cross-sectional index varies
  over all countries but the age component is fixed to one age group,
  here group 00.  In other words, each country-time unit is featured
  only once in the file.  Note also that the two column format is
  used.
\item  The type \texttt{``depvar.like''} is used to identify 
  any covariate that may simultaneously be used as the dependent
  variable and as a covariate.  For example, if you choose 
  \texttt{depvar=``cvds''}, then you may also choose to include 
  \texttt{``cvds''} in your vector of covariates.  In the case
  of forecasting mortality, you may also include any other cause 
  of mortality as a covariate as long as you identify it as being 
  of this covariate type.   
\end{enumerate}

\medskip
\noindent \emph{No Cross-Classifying Non-Geographic Nominal Variable.}
If there is not a cross-classifying non- geographic nominal
variable, i.e. if the argument \texttt{strata} is set to NA, the data
files will be in the two column format.  For covariates that are
grouped continuous variable dependent, designated by setting the type
equal to ``strata.independent'', the index will run over all
cross-sections.  This is the default value of the argument 
\texttt{cov.type} when there is not a strata.  For covariates that 
are grouped continuous variable independent, designated by setting 
the type in argument \texttt{cov.type} equal to ``strata.age.independent'', 
the index will run over only one level of the grouped continuous 
variable as before.  Covariates may not be declared to be of type
``age.independent'' when there is not a strata.

\paragraph{Dependent Variable}
The dependent variable data files must reside in the directory
specified by the argument \texttt{data.path} to \texttt{yourcast()}.
If you are supplying the dependent variable `as is' for
\texttt{yourcast()} to use, signaled by setting the argument
\texttt{population} to NA, there is only one data file for the
dependent variable.  For example, you may supply \texttt{yourcast()}
with mortality for each CSTS unit.  The name of the data file in this
case must be \texttt{``depvar''.txt}, where you supply the name
``depvar'' as the argument \texttt{depvar}.  If the dependent variable
is not being supplied `as is', i.e. if \texttt{yourcast()} will
generate it, signaled by setting the argument \texttt{population}
equal to a string, two files must be supplied.  In this case,
\texttt{yourcast()} will divide the values in the first file by the
values in the second file, generating a rate.  For example, you may
want \texttt{yourcast()} to calculate mortality given two data files,
one containing the number of deaths and the other containing the
population for each CSTS unit.  The name of the first data file, which
contains the numerator of the rate, is \texttt{``depvar''.txt}, where
``depvar'' is again the value you supply to the argument
\texttt{depvar}.  The name of the second data file, which contains the
denominator of the rate, is \texttt{``population''.txt}, where
``population'' is the value you supply to the argument
\texttt{population}.

The structure of each of these files is generally the same as that for
the covariates: by default they have a three column format with
columns value, index, and strata.  Each line must be white space
delimited and will correspond to a CSTS unit for a particular strata.
In other words, each CSTS unit will have $l$ lines in each file, where
$l$, as before, is the number of levels of the non-geographic nominal
index variable (otherwise known as strata), for a total of $l*n$ lines
in each file.  However, if there are no strata, you can set
\texttt{strata} to NA and store the data in files with only two
columns, index and value.  In this case, each line will correspond to
a CSTS unit and there will be a total of $n$ lines in each file.

%EV using depvar as cov
\paragraph{Dependent Variable as a Covariate} 
You may include the dependent variable, or any other variable
identified as being of this type, in the covariates vector. Covariates
of the dependent variable type undergo some transformations to avoid
neglecting large portions of the data for the actual dependent
variable time series due to missing values.  After finding the first
fully observed year for all covariates, \texttt{yourcast} fills any
subsequent non-fully observed years with a linear approximation of the
data.  Years that are not fully observed prior to the first fully
observed year will be deleted from the cross-section time series after
lagging the covariates by the number of years specified in
\texttt{lag}.  Any dependent variable values less than 0.5 are set
equal to 0.5.  \texttt{yourcast} also divides the dependent variable
time series by the \texttt{population} time series when a
dependent-variable-type covariate is given in conjunction with a
\texttt{population} file.  In addition, the same transformation is
applied to the dependent variable-as-covariate as is applied to the
actual dependent variable, which is specified with the parameter
\texttt{transform}. As noted above, including the natural \texttt{log}
of any dependent variable data file restricts \texttt{transform} to
values other than $1$ or $3$.

\paragraph{Files for Girosi-King Forecasting Method}
The following two files are used for the Girosi-King forecasting
method.  The first must be supplied in order to smooth over the
geographic nominal index variable.  The second must be supplied in
order to use a non-zero mean prior with your own mean profile.

The first is the file containing the data necessary to build the
adjacency matrix $s^{cntry}$ for the geographic nominal index
variable.  See sections 11.1 and 7.1 of \cite{GirKin04} for
information about this matrix.  The default name for this file is
\texttt{adjacency.txt}.  It resides in the directory specified by the
argument \texttt{prior.path}.  You may also supply your own filename
via the argument \texttt{Hct.c.deriv}.  If the string supplied to this
argument is a file name, the file must reside in the directory
specified by the argument \texttt{prior.path}.  Alternatively, you may
supply a complete path for the file.  Each line of this file contains
three white-space delimited entries.  The first and second entries
each contain a code for a level of the geographic nominal index
variable and the third contains a code denoting the similarity of
these two levels.  Each cross-sectional unit defined solely by the
geographic nominal index variable will have $c$ lines in the file,
where $c$ denotes the number of levels of the geographic nominal index
variable: one for its comparison with every other cross-sectional unit
(including itself).  The file will accordingly contain $c*c$ lines.
Note that each cross-sectional unit pair appears twice (permutations
are not deleted): the similarity between cross-sectional units $i$ and
$j$ is the same as that between cross-sectional units $j$ and $i$, for
$i \neq j$; each of these contribute one line to the file.  Similarity
between cross-sectional units is classified and coded as follows:
geographic neighbors, coded as 1; non-geographic neighbors, coded as
2; or non-neighbors (no similarity), coded as missing data
($-999.000$).  The similarity of a cross-sectional unit to itself is
classified as not similar (and accordingly coded $-999.000$).

The second is the file containing your own mean grouped continuous
variable profile for use in constructing a non-zero mean prior.  You
signal to \texttt{yourcast()} that such a file should be used by setting
the argument \texttt{zero.mean} equal to a string, which must be the
name of the file.  This file must be stored in the directory specified
by the argument \texttt{prior.path}.  When loaded into \texttt{R}, it
should yield a one column matrix where each entry contains the desired
mean for a level of the grouped continuous index variable.  The
matrix, in other words, should have dimension $a$-by-$1$, where $a$ is
the number of levels of the grouped continuous index variable.
Unfortunately, \texttt{yourcast()} does not support the scanning of a
file of arbitrary format and the converting of its content into such a
\texttt{R} matrix.  Instead, you need to create the matrix in
\texttt{R} and save this matrix to a file using the command
\begin{verbatim}
     > save(profile, file = ``myprofile.txt'', compress = FALSE)
\end{verbatim}
where ``profile'' is the name of the matrix you created and
``myprofile.txt'' is the name of the file.  You then must put this
file in the directory specified by \texttt{prior.path} and set the
value of the parameter \texttt{zero.mean} equal to ``myprofile.txt''.
Note that if you do not supply a file but instruct \texttt{yourcast()}
to use a non-zero mean prior by setting the parameter
\texttt{zero.mean} equal to FALSE, a mean profile will automatically be
generated by \texttt{yourcast()}.

\paragraph{Miscellaneous Files}
You may choose to supply a file that relates the codes and names
of the levels of the geographic nominal variable. 
This file should reside in the directory
specified by the argument \texttt{data.path} and is named
\texttt{``codes.names''.txt}, where ``codes.names'' is the
user-supplied value of the argument \texttt{codes.names}. Each level
of the geographic nominal index variable appears on one line for a
total of $c$ lines and the file is white-space delimited. It has a two
column structure, with the first column containing the code of a level and
the second column the name.  Note that supplying this file is
optional: both \texttt{yourcast()} and \texttt{yourgraph()} will function
without it.  Its only use is to produce output more amenable to
graphing (i.e., graphs that can contain names instead of codes).

\subsection{Forecasting Methods}\label{sec:fore}
\YourCast\ supports five methods for forecasting dependent variables
such as mortality, four of which use covariates (equation-by-equation
Poisson regression, equation-by-equation ordinary least squares, and
the two new Bayesian Girosi-King methods) and one of which does not
(Lee-Carter).

For the methods using covariates, forecasts are computed with the
one-step-ahead forecasting algorithm applied to data where the
covariates are lagged. First, dependent variable data for time $t$ is
associated with covariate data for time $(t - lag)$ using back-cast
covariates (see the discussion under the argument \texttt{lag} in section
\ref{sec:refyour}). Second, the desired model is estimated using the
lagged data, which posits a relationship between the dependent
variable at time $t$ and the covariates at time $(t - lag)$.  Third
and finally, covariate data from time $(t - lag)$ is plugged into the
estimated model to forecast a value of the dependent variable at time
$t$.  For example, to forecast mortality for the years 2001-2010 with
data on mortality from 1950-2000 and on covariates from 1920-2000
(note that covariates have been back-cast for the years 1920-1949),
\texttt{lag} is set equal to 10 and mortality data at time $t$ is
associated with covariate data at time $(t - 10)$, e.g. mortality data
in 2000 with covariate data in 1990 and mortality data in 1950 with
covariate data in 1940.  The desired model for forecasting is
estimated using the lagged data set:  mortality data from 1950-2000 and
covariate data from 1940-1990.  Mortality is then forecast for the
years 2001-2010 by plugging covariate data for the years 1991-2000
into the estimated model.  See the discussion in section 3.1.3 of
\cite{GirKin04} for more information.

For the method without covariates, forecasts are computed by
estimating the model parameters and than forecasting one of them,
$\gamma$, using a standard univariate time series model. (The other
parameters are assumed constant over time.)  See section 2.6.3 of
\cite{GirKin04} for further information.

\subsubsection{Lee-Carter}\label{sec:forelc}
A forecasting method that does not use covariates.  Instead, it seeks
to reduce the dimensionality in the data for each level of the
geographic nominal index variable using the nonparametric tool of
principal components analysis.  In the case of forecasting mortality,
the underlying structure is the known smoothness of log-mortality
over age groups.  Note that this forecasting method will probably be
of limited utility for non-demographic dependent variables and even
potentially for demographic dependent variables other than all-cause
mortality.  See section 2.7 of \cite{GirKin04} for a discussion of the
limitations of this method.  Further details are discussed below; for
additional information, see section 2.6 of \cite{GirKin04}.

The model parameters returned in the forecast object and saved in the
output file (as the element and object \texttt{coeff}, respectively)
are $\gamma$ and $\beta$ (as a list), organized by level of the
geographic nominal index variable.  $\beta$ is the first principal
component, the normalized eigenvector associated with the largest
eigenvalue of the covariance matrix of the data.  The in-sample
$\gamma$s, a t-by-1 vector, are the projection of the grouped
continuous variable profiles onto the first principal component.  The
predicted values for a time $t$ are calculated by the product of
$\gamma_t$ and $\beta$ plus the mean grouped continuous variable
profile, $\alpha$, where the out-of-sample $\gamma$s are forecast
using the random walk with drift model.  Standard errors are neither
calculated nor returned:  the object \texttt{std} of the output file
and corresponding element of the forecast object are both assigned the value
NULL.  Actual and predicted values of the dependent variable are
returned on the user-selected transform scale.

\subsubsection{Poisson}\label{sec:forepo}
A forecasting method that uses covariates.  An equation-by-equation
approach, which analyzes each time series (i.e. each CS
unit) using a generalized linear model with Poisson error distribution
and log link.  Note that the argument \texttt{transform} must be equal
to 1 (i.e., the log) in order to use the Poisson method for
forecasting.  Further details are discussed below; see section 3.1.1
of \cite{GirKin04} for additional information.

Collinear covariates, which result in undefined coefficients (assigned
the value NA by the glm function in R), are eliminated as part of the
forecasting algorithm.  This action is controlled by the hardwired
parameter \texttt{delta.tol}.  To eliminate this portion of the
algorithm, i.e. to \textit{not} eliminate collinearities, alter the code of
\texttt{yourcast()} to set \texttt{delta.tol} equal to zero, in which
case undefined coefficients will merely be set to zero.  The degrees
of freedom for each cross-section is also calculated.  If it is too
small (i.e., if data is sparse relative to the number of covariates),
the coefficients for that cross-section are assigned the value NA.

The model parameters returned in the forecast object and saved in the
output file (as the element and object \texttt{coeff}, respectively)
are the estimated coefficients.  Standard errors for these
coefficients are not provided:  the object \texttt{std} of the output
file and corresponding element of the forecast object are both assigned the
value NULL).  Both actual and predicted values of the dependent
variable are on the scale of the linear predictors, e.g.
log-mortality in the case of forecasting mortality.

\subsubsection{Least Squares}\label{sec:forels}
A forecasting method that uses covariates.  Like the Poisson method,
an equation-by-equation approach, but which unlike the Poisson uses
ordinary (unweighted) linear regression to analyze each time series.
Details follow below; for more information, see section 3.1.2 of
\cite{GirKin04}.

The model parameters returned in the forecast object and saved in the
output file (as the element and object \texttt{coeff}, respectively)
are the estimated coefficients.  The estimated standard deviations,
the square root of the mean squared error, are returned in the
forecast object and saved in the output file (as the element and
object \texttt{std}, respectively).  Actual and predicted values of
the dependent variable are returned on your selected transform
scale, expressed by the value of \texttt{transform}.

\subsubsection{Bayesian Girosi-King}\label{sec:foregk}
A forecasting method that uses covariates.  This Bayesian approach
reduces inefficiency in model estimation relative to equation-
by-equation linear regression by ``borrowing strength'' for one cross-
section from others.  Partial pooling of cross-sections is
accomplished by requiring similar cross-sections to have similar but
not identical expected values on the dependent variable, unlike
conventional Bayesian methods that require similarity in coefficients.
Beneficial features of this approach are that different explanatory
variables are allowed in each cross-section and subject matter
expertise is directly incorporated into models.  There are two
supported methods for implementing this model.  The first, represented
by the option ``Bayes'' to the argument \texttt{model}, uses the Gibbs
sampling algorithm to draw samples from the posterior density.  This
is a Markov Chain Monte Carlo technique.  See chapter 9 of
\cite{GirKin04} for further information.  The second, represented by
the option ``MAP'' to the argument \texttt{model}, uses the maximum of
the posterior density as its estimate.  This is known as the Maximum A
Posteriori technique.  It does not require Gibbs sampling; instead, it
simply requires an iterative algorithm that can solve a maximization
problem.  This algorithm is faster than the Gibbs sampler.  See
chapter 10 of \cite{GirKin04} for further information.
%%  Is the MAP estimator implemented by yourcast the MAP,
%%  the marginal MAP, or the conditional MAP?  I've written
%%  as if it were the MAP.
Details of this forecasting model follow below; for more information,
see chapters 4-8 of \cite{GirKin04}.

Building upon least squares regression, the log of the dependent
variable is assumed to be distributed normally with mean $\mu$ and
variance $\sigma^2$.  The mean $\mu$, the expected value of the
dependent variable, is a linear function of the covariates (i.e., $\mu
= {Z\beta}$, where $Z$ is the matrix of covariates and $\beta$ the
vector of coefficients).  The coefficients $\beta$ and standard
deviations $\sigma$ are assumed to be random variables with their own
prior distributions, $\Pr(\beta|\theta)$ and $\Pr(\sigma)$,
respectively.  $\theta$ is a hyper or smoothing parameter with its own
prior distribution, $\Pr(\theta)$.  Standard Bayesian analysis defines
the overall prior, $\Pr(\beta, \theta, \sigma)$, as $\Pr(\beta|\theta)
\Pr(\theta) \Pr(\sigma)$.  The Girosi and King method modifies the
standard Bayesian approach as follows.  First, a prior is put on the
dependent variable, $\Pr(\mu|\theta)$: the expected value of the
dependent variable is smoothed over the cross-sections and time, i.e.
over any combination of the grouped continuous index variable, the
geographic nominal index variable, and time.  Second, the prior on the
expected value of the dependent variable is translated into a prior on
the coefficients, $\Pr(\beta|\theta)$, using the relationship $\mu =
Z\beta$ and plugged into the equation for the overall prior.

Note that in order for the prior to remain improper and hence
indifferent to certain patterns in the dependent variable, there must
be at least one covariate that is included in all cross-sections.
Including a constant in the model satisfies this criterion.  Note also
that for smoothing over the geographic nominal variable, the adjacency
matrix $s^{cntry}$ must be computed by hand.  Such a matrix has been
computed for forecasting mortality; you may either provide your own
or modify the existing matrix.  See section \ref{sec:datafil}.
Note finally that the method for imputing zero values of the
dependent variable described in section 6.5 of \cite{GirKin04} is not
currently implemented.  At the moment, observations below 0.5 are 
set equal to 0.5 during data processing to eliminate zero values.

%%  The method of adding 0.5 to eliminate zero values needs to undergo 
%%  changes to accommodate the case when the users supply the dependent
%%  variable as one data file (argument depvar).
%% EV I have changed from adding 0.5 to all death data to simply set 
%% equal to 0.5 any death that is below 0.5, as death[death <0.5] <- 0.5 
%%I hope this help in accuracy.  

The model parameters returned in the forecast object and saved in the
output file (as the element \texttt{coef}) are the estimated
coefficients.  Estimated standard errors are not returned:  the
object \texttt{std} of the output file and corresponding element of
the forecast object are both assigned the value NULL.  Actual and predicted
values of the dependent variable are returned on your selected
transform scale, the value of the parameter \texttt{transform}.

Users of this method must set a few parameters in order to construct
the overall prior.

\paragraph{Parameters related to the prior for $\theta$.}  Here, $\theta$ 
is treated as a random variable with its own proper prior distribution. It 
is best viewed as a set of smoothing parameters that control the moments
(e.g., variance) of the density $\Pr(\mu|\theta)$.  Each component of
a mixed smoothness functional with respect to one index variable or
interaction has its own smoothing parameter.  In general, larger
values of $\theta$ correspond to smoother predictions and less
influence for the data.  Knowledge about the quantities that $\theta$
determines is inverted to determine knowledge about $\theta$.
Specifically, the average variability of the expected values of the
dependent variable over the levels of the cross-sectional or time
variable being smoothed in $\Pr(\mu|\theta)$ is a known function of
$\theta$ ($\theta = 1/\sigma^{2}$).  Your choice of an acceptable overall
range of variability leads to a range of acceptable values of $\theta$
(and to a prior distribution for $\theta$).  More simply, your choice of an
acceptable variance leads to a choice for $\theta$.  For example, an
average variance of 0.09 (an average standard deviation of 0.3) over
age groups works well in the case of forecasting mortality.  These
choices are controlled by the arguments of form \texttt{H*.sigma}
(e.g., \texttt{Ht.sigma}) to \texttt{yourcast()}.  Note that setting one
of these arguments equal to NA sets $\theta$ for the corresponding
component of the mixed smoothness functional equal to zero.  That
component accordingly has no effect on the prior.  If you set all of the
\texttt{H*.sigma} arguments equal to NA, the prior for the
expected value of the dependent variable has infinite standard
deviation and the likelihood does all of the work.  This is empirical
Bayes.  The arguments \texttt{H*.sigma} can accordingly also be
viewed as representing how certain you are about your prior knowledge.  
See sections 4.1.1, 4.5.3, and 6.2 of \cite{GirKin04}.
  
\paragraph{Parameters related to the prior for $\mu$, which is translated into 
  the prior for $\beta$.}
  
  \begin{enumerate}
  \item You may decide if the prior is to be for the expected value
    of the dependent variable or for its deviation from some mean
    profile.  The former prior has zero mean (it is symmetric and
    centered about the origin); the latter prior has mean $\bar{\mu}$,
    which is obtained by centering the dependent variable around this
    mean $\bar{\mu}$ and then using a zero mean prior.  In other
    words, you must decide if you wish to smooth over either the
    expected value of the dependent variable or its deviation from the
    mean.  This choice is controlled by the argument
    \texttt{zero.mean} to \texttt{yourcast()}.  At the moment, the mean
    $\bar{\mu}$ is either read from a file that you provide or
    automatically generated by \texttt{yourcast()}.  See section
    \ref{sec:datafil} for information about this file.  Note that
    there are various ways of calculating $\bar{\mu}$ (over all
    cross-sections, over a given cross-section, etc.).  The default
    approach used by \texttt{yourcast} is to calculate the average
    over all levels of the geographic nominal variable and time for
    each level of the grouped continuous variable (i.e., over all
    cross-sections), generating a mean
    age profile in the case of forecasting mortality.  See sections
    4.51, 5.24, 5.26, and 6.2 of \cite{GirKin04}.
    
  \item You may decide over which index variables the prior should
    smooth.  That is, should the prior smooth the level of the
    expected value of the dependent variable over the grouped
    continuous index variable, the geographic nominal index variable,
    the time index variable, or any combination of these variables?
    See chapter 5 and sections 7.1, 7.2, and 7.3 of \cite{GirKin04}.
    This choice is controlled by the arguments \texttt{Ha.sigma};
    \texttt{Ht.sigma}; and \texttt{Hct.c.deriv} and
    \texttt{Hct.t.deriv}, which control the grouped continuous, time,
    and geographic nominal variable components, respectively.
    Additionally or alternatively, it is possible to smooth the time
    trend over the levels of the grouped continuous or geographic
    nominal variables. See sections 7.4 and 7.5 in \cite{GirKin04}.
    The former choice is controlled by the argument \texttt{Hat.sigma}
    and the latter by the arguments \texttt{Hct.c.deriv} and
    \texttt{Hct.t.deriv}.  In the special case of the geographic
    nominal index variable, smoothing is possible over either the
    level or the time trend of the expected value of the dependent
    variable, not both.  To smooth the level, set \texttt{Hct.c.deriv}
    equal to TRUE or a string and set \texttt{Hct.t.deriv} equal to 1;
    to smooth the time trend, set \texttt{Hct.c.deriv} equal to TRUE
    and set \texttt{Hct.t.deriv} equal to c(0, 1) (or to a
    higher-order partial derivative).  To smooth over neither, set
    \texttt{Hct.c.deriv} equal to FALSE.  For all other variables, the
    \texttt{H*.sigma} argument(s) corresponding to the variable(s) and
    interaction(s) over which you would like to smooth the
    dependent variable should be set to a finite and small number; all
    other \texttt{H*.sigma} arguments, those corresponding to the
    variables and interactions over which you do not wish to
    smooth the dependent variable, should be set equal to NA.  For
    example, to only smooth the level of the dependent variable over
    the grouped continuous index variable, set \texttt{Ht.sigma} and
    \texttt{Hat.sigma} equal to NA and \texttt{Hct.c.deriv} equal to
    FALSE.
  
  \item You may choose the weights on the derivatives of each
    component in the mixed smoothness functional.  The discussion that
    follows is couched in terms of either the smoothness functional
    for the prior when only smoothing over one index variable or the
    smoothness functional where the time derivative replaces the level
    when smoothing a time trend.  The form of the functional, the
    order of derivative(s), and the relative weight of the individual
    smoothness functions in the sum if a mixed functional is used (the
    parameter $\lambda$ in the equation on p. 133 of \cite{GirKin04})
    are controlled by the arguments \texttt{Ha.deriv},
    \texttt{Ht.deriv}, and \texttt{Hat.a.deriv}.  Each of these
    arguments takes a vector, each element of which corresponds to the
    weight put on the derivative with order corresponding to its index
    position minus one.  Accordingly, the first element in the vector
    corresponds to the weight on the 0th order derivative, the second
    element to the weight on the 1st order derivative, etc.  Note that
    the 0th order derivative should usually be assigned a weight of
    zero.  (Using a derivative of order zero imposes a constraint on
    the norm of the function, which is not generally wise.)  To use a
    standard smoothness functional, put a weight on only one
    derivative.  For example, c(0, 0, 1) uses only a second order
    derivative and c(0, 1) and c(0, 1, 0) both use only a first order
    derivative.  To use a mixed smoothness functional, put weights on
    more than one derivative.  For example, c(0, 1, 1) uses both first
    and second order derivatives and weights them equally.  You only
    need to set the weights for the index variables or interactions
    over which you are smoothing; for example, set \texttt{Ht.deriv}
    and \texttt{Ha.deriv} if smoothing over the levels of the grouped
    continuous and time variables.
    
    In general, the order $n$ of the derivative controls the degree of
    smoothness.  Specifically, it expresses the form of prior
    indifference: using the first derivative expresses indifference to
    the absolute level taken by the dependent variable; using the
    second derivative expresses indifference to the mean and trend of
    the dependent variable over levels of the index variable.
    Additionally, as the order of the derivative increases, the
    expected values of the dependent variable become locally smoother,
    i.e. the values at different levels of the grouped continuous
    variable become more correlated with each other; however, there
    are also more global `bumps'.  A mixed smoothness functional, the
    sum of $k$ standard smoothness functionals, allows separate
    control of these characteristics: the size of the null space is
    controlled by the lowest order derivative, while the degree of
    local smoothness is controlled by the highest order derivative.
    In principle, there is no upper bound on the order of derivative
    that can be used.  In practice, the maximum degree of derivative
    is proportional to the number of levels of the grouped continuous
    or geographic nominal index variables and to the length of the
    time series, in accordance with standard numerical analysis.  For
    example, when forecasting mortality with five age groups, you 
    should probably not use a derivative of more than order three.  A
    mixed smoothness functional that is the sum of two standard
    smoothness functionals ($k = 2$), one with the lowest and one with
    the highest order derivative desired, will suffice for most users.
    However, you may construct mixed smoothness functionals with
    more than two derivatives (i.e., $k > 2$) if you so desire within
    the limits imposed by numerical analysis.  Choosing the weights on
    the derivatives will likely involve trial and error.  You should
    try a variety of weights; generate plots of draws from the
    resulting priors; and choose the weights that produce the best
    looking priors.  Most users will not need a mixed smoothness
    functional.  For example, in the case of forecasting mortality,
    reasonable results are obtained with a standard smoothness
    functional using only a second order derivative.  See sections 5.2
    and 6.1 of \cite{GirKin04}.
    
    Smoothing either the level or the time trend over the geographic
    nominal variable is again a special case: only the use of the
    first derivative is supported (a standard functional).
    Accordingly, because \texttt{Hct.c.deriv} is not numeric, you cannot 
    have control of the weights as described above.  Additionally, the
    arguments \texttt{Hat.t.deriv} and \texttt{Hct.t.deriv} determine
    the time derivative that replaces the level in the smoothness
    functional when smoothing a time trend over either the grouped
    continuous or geographic nominal index variables; a standard
    functional containing the first or higher-order partial derivative
    (with respect to time) is allowed, e.g., c(0, 1) or c(0, 0, 1).
 
  \item You may choose the measure in the mixed smoothness
    functional integral.  This parameter allows for variance in how
    the lack of smoothness is penalized over the levels of the
    cross-sectional index variables and time.  The integral can be
    viewed as averaging over groups or time according to the measure.
    You have three classes of options.  First, groups or time can be 
    weighted equally (a uniform measure).  Second, they can be weighed
    proportional to the level of the group or time (measure equal to
    $x^{weight} dx$, where $x$ denotes the level of the group or time
    period and $weight$ a scalar, which penalizes larger levels more
    than smaller levels).  Third, they can be weighed by any other 
    weighing scheme, expressed by a vector
    with each element as the weight for a level, which is taken as the
    discretization of the measure.  For example, for age groups 1 to
    17 in forecasting mortality, a non-uniform measure $a^{l}da$, $l =
    1, 2,\dots,17$ penalizes older ages more than younger ones for
    lack of smoothness, resulting in a prior that will oscillate more
    at younger ages than at older ones.  Note that the argument for a
    non-uniform measure changes with a non-zero mean prior.  Returning
    again to the example from forecasting mortality, the deviation
    from the mean might not be thought to be larger at younger than at
    older ages; knowledge might be thought to be better at older than
    at younger ages, however, so a prior with more variance at younger
    than at older ages might be desirable.  As with the weights on the
    derivatives in the mixed smoothness functional, the recommended
    strategy for choosing the measure is to generate plots of draws
    from priors with varying measures and then choose the prior that
    best represents your prior knowledge.  This choice is
    controlled by the arguments of form \texttt{H*.time.weight},
    \texttt{H*.age.weight}, and \texttt{H*.cntry.weight} to
    \texttt{yourcast()}.  See sections 5.2.1, 5.2.2, and 6.3 in
    \citet{GirKin04}.
  \end{enumerate}

\paragraph{Parameters related to the prior for $\sigma$.}  There are actually
no arguments to yourcast that directly govern the choice of these
parameters.  Nevertheless, the prior for $\sigma$ is discussed here
because both the modeling and implementation strategies chosen may not
be appropriate for certain applications. Like $\theta$, $\sigma$ is
treated as a random variable with its own proper prior distribution.
The observation that when the number of events is not too small, the
variance of the log of the dependent variable is inversely
proportional to the expected value of the dependent variable leads to
modeling $\sigma_i$ as inversely proportional to the square root of an
average historical level of the value of the dependent variable for
cross-section $i$.  (The dependency of the standard deviations on time
is ignored; this assumption may be less tenable in applications other
than forecasting mortality.)  The implementation estimates the average
historical level of the dependent variable by a run of OLS; the
predicted values of the dependent variable for each observation are
averaged over time in each cross-section.  It adds to the inverse of
the square root of this quantity the averaged (over both time and the
grouped continuous variable) estimated standard deviation from OLS in
each cross-section minus the averaged (over both time and the grouped
continuous variable) predicted values of the dependent variable from
OLS in each cross-section.  See section 6.5 in \citet{GirKin04}.

%%  The implementation almost certainly does not follow this description,
%%  which is taken from the book.  Months ago, Federico mentioned that 
%%  this was all in flux.  Elena and I have discussed some parameters
%%  related to the prior for \sigma that may or may not need to be
%%  user-controlled:  2 have recently been made user-controlled instead of
%%  hard-coded.  We need to fully
%%  describe these parameters and then to update the discussion
%%  above and relate the implementation to the book.

\subsection{Graphics}

You can produce graphs of both the forecasts generated by
\texttt{yourcast} and the actual dependent variable data using the
function \texttt{yourgraph}.  This function takes as its input either
the output file from \texttt{yourcast}; the object returned by
\texttt{yourcast}; or a file or object formatted to the same
specifications, as described in section \ref{sec:refyour}.
Specifically, the object you supply to \texttt{yourgraph()}
must contain the following objects from \texttt{yourcast()}'s
forecast output:  ``yhatin'', ``yhatout'', ``insampy'',
``outsampy'', ``depvar'', ``strata'', ``model'', ``out.path'', and 
``cntry.lst''.

For every level of the geographic nominal variable, you may choose to
generate up to two graphs.  By setting the parameter
\texttt{plot.time.series} equal to the value TRUE, you signal to
\texttt{yourgraph} that it should plot a time series of the data for
each level of the grouped continuous index variable on the graph, with
one such graph being generated for each level of the geographic
nominal index variable.  In the case of forecasting mortality, this
allows you to plot a set of time series for each country on the same
graph, where the set of time series corresponds to the $17$ age groups.
One graph is generated for each country.  By setting the parameter
\texttt{plot.age.profiles} equal to the value TRUE, you tell
\texttt{yourgraph} to plot the data against the level of the grouped
continuous index variable for each time period, with one such graph
again being generated for each level of the geographic nominal index
variable.  In the case of forecasting mortality, this allows you to
plot a set of age profiles of the data for each country on the same
graph, where the set is now the years included in the analysis.  You
control exactly which data is plotted via four parameters.  Setting
the arguments \texttt{obs.insample} and \texttt{obs.outsample} equal
to TRUE plots observed data for the in-sample and the out-of-sample
periods, respectively.  Similarly, setting the arguments
\texttt{pred.insample} and \texttt{pred.outsample} equal to TRUE plots
the forecasts for the in-sample and the out-of-sample periods,
respectively.  The default value of all of these parameters is TRUE.
The parameter \texttt{dv.transform} allows you to specify a
transformation of the observed data and forecasts.  Its default value
is the identity function, i.e. to plot the data ``as is''.

A variety of parameters control the layout of the graphs, such as the
number of rows into which the graphs are organized and the background
color.  Of special note is the argument \texttt{print2file}.  Setting
this parameter equal to the value TRUE prints the graphs to an
encapsulated postscript file and FALSE to the screen.  In the former
case, this file is saved to the directory you specify via the
parameter \texttt{out.path}, which by default is the sub-directory
``/OUTPUT'' of your working directory.  See section \ref{sec:refgraph}
for a complete list of the graphical layout parameters.

\section{Reference}

\subsection{Function `yourcast'}\label{sec:refyour}

This function performs any necessary data processing.  It then uses
the processed data to forecast a dependent variable such as mortality
according to the method you have chosen.  You may supply parameters
either as arguments to the function, following conventional practice,
or in a file, as discussed below. 

\subsubsection{Usage}

\begin{verbatim}
  my.forecast <-  yourcast(userfile = NULL, allcauses= ``allc'', 
                  depvar = ``allc'', population = ``population'', 
                  cov = c(``cnst'', ``time''), cov.type = NA,
                  strata = NA, userages = NA, usercntrylist = NA,
                  skip = 8, cov.select = NA, age.select = NA,
                  transform = 1, model = ``OLS'', fore = 2030,
                  yrest = 2000, lag = 30, svdtol = 10^-10, 
                  save.output = TRUE, save.FULL = FALSE, 
                  reuse.data = TRUE, reuse.path = ``OUTPUT/'', 
                  prior.path = ``DATA/'', data.path = ``DATA/'', 
                  cov.path = ``DATA/'', out.path = ``OUTPUT/'', 
                  codes.names = NA, lag.cutoff = 1, tol = 0.99,
                  elim.collinear = TRUE, standardize = FALSE,
                  cntry.digits = 4, year.digits = 4, age.digits = 
                  2, digit.first = 0, zero.mean = FALSE, 
                  Ha.deriv = c(0, 0, 1), Ha.age.weight = 0, 
                  Ha.time.weight = 0, Ha.sigma = 0.3,
                  Hat.a.deriv = c(0, 0, 1), Hat.t.deriv = c(0, 1),
                  Hat.age.weight = 0, Hat.time.weight = 0, 
                  Hat.sigma = 0.2, Ht.deriv = c(0, 0, 1), 
                  Ht.age.weight = 0, Ht.time.weight = 0,
                  Ht.sigma = 0.3, Hct.c.deriv = TRUE,
                  Hct.t.deriv = 1, Hct.cntry.weight = 0,
                  Hct.time.weight = 0, Hct.sigma = 0.3)
\end{verbatim}
Note: the actual function definition assigns the value NULL to each parameter
instead of the default values, which are contained in an internal function.  
The default values are listed here for your convenience, and can be viewed 
by invoking the helper function \texttt{args.default()} in your working 
environment.

\subsubsection{Inputs}

\begin{description}
\item[userfile] A string; the name of the file that contains
  your values for \texttt{yourcast()}'s arguments, one of the two
  methods of input.  This file contains R code that performs
  assignments of your desired values to the arguments of
  \texttt{yourcast()}.  An example of the contents of the file follows
  below.
  \begin{verbatim} 
    lag <- 30
    usercntrylist <- c(2450, 2090)
  \end{verbatim} 
  Each assignment should appear on its own line or be separated by a
  semi-colon.  The file can either be stored in your working
  directory, in which case the argument should be equal to the name of
  the file, or in any other directory, in which case the argument
  should be equal to the complete path of the file.  Default is NULL.
\end{description}

\noindent \emph{Parameters related to data structure and model:}

\begin{description}
\item[depvar] A string; the name of the dependent variable to
  forecast.  The input data file for the dependent variable, which has
  name \texttt{``depvar''.txt}, contains three columns (value, index
  and strata) unless the argument \texttt{strata} is set to NA, in
  which case the data file contains two columns (value and index).
  If you do \emph{not} supply the values of the dependent variable
  `as is' for \texttt{yourcast()} to use, signaled by setting the
  argument \texttt{population} equal to a string, then 
  \texttt{yourcast()} will divide the values in the file 
  \texttt{``depvar''.txt} by the values in the file
  \texttt{``population''.txt} to generate the dependent variable.  
  See the discussion under \texttt{population} for more details.
  Default is set to the value of parameter \texttt{allcauses} or, if
  the latter is NA, then to ``allc'', which stands for all-cause mortality.

\item[allcauses] A string that contains the name of the data file containing
  all-cause mortality. It is useful to have this parameter,  
  which the simulation can recognize among your choices of covariates, 
  if you decide to include it in the covariate vector 
   without providing a type for it 
  (or just type NA, the default \texttt{cov.type}). 
  It can be set to NA if you want to ignore this 
  extra parameter and treat all causes of mortality as any other 
  dependent variable data file. Default is ``allc''.

%%  HMS:  Does yourcast recognize the cov.type of allc, both as a dependent
%%  variable and as a covariate?

\item[population] A string; the name of the variable
  used in conjunction with the variable supplied to the argument
  \texttt{depvar} to generate a `rate' dependent variable. 
  If the argument is set to NA, the dependent variable is
  supplied solely as a single file as specified in argument \texttt{depvar}.
  If the argument is equal to a string, \texttt{yourcast()} uses the values
  in both the files \texttt{``depvar''.txt} and \texttt{``population''.txt}
  to generate the dependent variable, where ``depvar'' and ``population''
  are the arguments supplied to \texttt{depvar} and \texttt{population},
  respectively.  Specifically, \texttt{yourcast()} divides the values in 
  the file \texttt{``depvar''.txt} by the values in the file 
  \texttt{``population''.txt} during data processing.  When forecasting 
  mortality, for example, the number of deaths in and the population of 
  each CSTS unit may be supplied via the arguments 
  \texttt{depvar} and \texttt{population}, respectively, which 
  \texttt{yourcast()} will combine (dividing the former by the latter) to 
  generate mortality. The input data file for the population values
  contains three columns (value, index and strata) unless the argument 
  \texttt{strata} is set to NA, in which case the data file contains 
  two columns (value and index).  It resides in the directory specified
  by \texttt{data.path}.  Default is ``population''.
  
\item[cov] A string vector; each element of the vector is a string
  that specifies a covariate to include in the models for forecasting
  methods that use covariates. To include time (as specified by the time 
  component of the CSTS index) as a covariate, add the string
  ``time'' to the vector.  To include a constant, add the string
  ``cnst'' to the vector.  To include a lag of the dependent
  variable among the covariates, add the string ``depvar'' to the
  vector of covariates, where ``depvar'' 
  is the name of the dependent variable data file supplied 
  to argument ``depvar''.  You may also use any other variable and data
  file of the dependent variable type as a covariate, in addition to
  the one you have chosen as the argument of ``depvar''. 
 
  Covariate names (including ``time'') may
  have a $ln$ operator such as ``ln(tobacco)'' or ``ln(allc)'', 
  which instructs \texttt{yourcast()} to take the natural log of 
  the covariate; they may also have a power such as ``(tobacco)3'', 
  or ``(cvds)2'', which instructs \texttt{yourcast()} to elevate the 
  covariate to the power of 3 and 2, respectively.  
  Note that if you include power transformations of covariates, 
  \texttt{yourcast()} will automatically include 
  lower-order terms if you have not specified their inclusion.  
  For example, if ``(tobacco)3'' is included, \texttt{yourcast()} will add 
  both ``tobacco'' and ``(tobacco)2'' to the model if you did not
  specify them.  The $ln$ and the power operators may be
  combined. For example, to use the squared logarithm of gdp as a 
  covariate, you should include ``ln(gdp)2'' as an element of the
  vector.  Note that this will \emph{not} be interpreted as a log of 
  a power transformation, since this would lead to
  a multiple of ``ln(gdp)''.  Note also that lower-order terms of logged
  covariates will not be automatically included; you must
  explicitly specify them in the list of covariates if you want them
  in the model.  For example, if you include ``ln(gdp)2'' as
  an element of the vector, ``ln(gdp)'' will only be included in the
  model if you also supply it.  

  All of these rules apply equally to any covariate of type ``dependent 
  variable'', so you may include its powers, its natural log, and 
  powers of its natural log.  Nevertheless, supplying the natural log of 
  the dependent variable, or of any covariate of this type, 
  is subject to the restriction that the parameter \texttt{``transform''} 
  must not be equal to $1$ or $3$.  The simulation will stop and show a 
  warning message if both \texttt{transform} is equal to either $1$ or $3$ 
  and the vector of covariates contains any log of a dependent variable 
  type of covariate.  Default is c(``cnst'', ``time'').
  
\item[cov.type] A list of covariate names (a subset of option
  \texttt{cov}) and their types, with the following syntax:
  list(cov.name1 = cov.type1, cov.name2 = cov.type2,...). The type of a
  covariate describes whether it is strata and/or grouped continuous
  variable independent or if it is of the dependent variable type. 
  Four types are available: ``strata.independent'', 
  non-geographic nominal variable independent;
  ``age.independent'', grouped continuous variable independent; 
  ``strata.age.independent'' or ``age.strata.independent'' (either format
  is accepted), non-geographic nominal and grouped
  continuous variable independent; and ``depvar.like'', those 
  covariates that may also be arguments of the parameter ``depvar''.
  
  If a covariate from \texttt{cov} is either not included in this 
  argument or is included and set to NA, 
  \texttt{yourcast()} will treat it as strata and
  grouped continuous variable dependent; however, if \texttt{strata} is set to
  NA, the covariate is treated as grouped continuous
  variable dependent and strata independent. In addition, you may also set 
  ``depvar'' to type NA or not include its type at all and similarly  
   for ``allc''. (For the latter, the parameter ``allcauses'' 
   is set to ``allc'', or otherwise you also need 
   to identify ``allc'' with type ``depvar.like''.)  Thereafter, 
   the program will recognize they belong to the special type 
  ``depvar.like'' and treat them accordingly with the transformation 
   that were discussed in section \ref{sec:datafil}. 
   Any other covariate that may be of the dependent variable 
  class need to be identified  with the \texttt{cov.type} of ``depvar.like''. 
  
  Covariate types do not need to be supplied for the constant, time, 
  the lag of the dependent variable, and power or log transformations 
  of covariates.  The input data file format for a covariate 
  depends upon its type.  Covariates
  specific to strata and groups of the grouped continuous variable are
  stored in files with 3 columns: value, index and strata, where the
  index runs over all the cross-sections. However, if you 
  specify that the value of a covariate does not depend on the
  strata and/or grouped continuous variable groups, the covariate is
  stored in a different format.  If the type is set to
  ``strata.independent'', the data files have only two columns, value and
  index, with the index running over all cross-sections.  If the
  type is set to ``age.independent'', the data files have three
  columns as before but the index runs over only one level of the
  grouped continuous variable.  If the type is set to
  ``strata.age.independent'', the data files have only two columns as
  before and the index runs over only one level of the grouped
  continuous variable.  

  If the argument \texttt{cov.type} itself is set to NA, 
  and \texttt{strata} is not NA, then all covariates are assumed to be 
  strata and grouped continuous variable dependent, except for the 
  covariates ``depvar'' and ``allc'', provided that for the latter 
  \texttt{``allcauses''} is equal to \texttt{``allc''}.  
  Both will be recognized to be of type \texttt{``depvar.like''}.  
  When argument \texttt{strata} is set to NA, then all
  covariates are assumed to be strata independent and grouped
  continuous variable dependent or of type ``depvar.like'', only for the 
  cases of the dependent variable itself and ``allc''.  See the argument 
  \texttt{strata} for further information.  Default is NA.

%% Re-work this above:  not clear

\item[strata] An integer; a discrete code identifying a subset of data
  to use for the analysis.  The strata are the levels of a
  non-geographic nominal variable that may define the cross-sectional
  structure of the data in conjunction with the grouped continuous and
  geographic nominal variables.  If such a variable exists,
  information about the strata appears in the third column of the data
  files for the covariates and dependent variable.  For example, when
  forecasting mortality, the non-geographic nominal variable is gender
  with levels ``males'' (coded as $2$) and ''females'' (coded as $3$);
  mortality forecasts are generated for one of these strata using the
  appropriate subset of the data.  If the argument is set to NA, a
  non-geographic nominal variables does not cross-classify the data
  and no third column should be included in the input data files.  In
  this case, the options for the argument \texttt{cov.type} are
  limited to two, ``strata.independent'' for covariates that are
  grouped continuous variable dependent and ``strata.age.independent''
  for covariates that are grouped continuous variable independent.

  Setting \texttt{cov.type} equal to NA when \texttt{strata} is equal
  to NA instructs \texttt{yourcast()} to assume that covariates are
  grouped continuous variable dependent, as does not supplying a
  covariate from \texttt{cov} in the argument \texttt{cov.type} or
  setting the type equal to ``strata.independent''; in all of these
  cases, \texttt{yourcast()} will look for a data file with two columns,
  index and value, with the index running over all cross-sections.  If
  the type is set to ``strata.age.independent'', the index will not
  run over all cross-sections (rather, it will run over only one level
  of the grouped continuous variable) but will still contain two
  columns.  Supplying the covariate type ``age.independent'' will
  return an error message in this case.  Default is NA.
  
\item[userages] Either a numeric vector where each element of the
  vector is a level of the grouped continuous variable to include in
  the analysis or a string that is the name of a file that contains
  this information.  If the file resides in the your working
  directory, only the name of the file is supplied; if it resides
  elsewhere, the complete path must be specified.  Setting this
  argument equal to NA does not subset by (i.e., uses all supplied
  levels of) the grouped continuous index variable unless a file
  called \texttt{``depvar''.ages} exists in the directory
  \texttt{data.path}, in which case the program uses the information
  in this file to set the value of the argument.  The code for each
  level to forecast must appear on its own line.  For example, in the
  case of forecasting mortality for the dependent variable of breast
  cancer, forecasts may be desired for relevant age groups such as
  $50$--$55$ but not for irrelevant age groups such as $10$--$15$.  Hence, the
  file \texttt{brst.ages}, if breast cancer is represented to the
  argument \texttt{depvar} as \texttt{``brst''}, might contain the
  integers $35$, $40$, $45$, $50$, $55$, $60$, $65$, and $70$ (each the code
  representing a relevant age group and each appearing on its own
  line). Default is NA.

\item[usercntrylist] Either a numeric vector where each element of the
  vector is a level of the geographic nominal index variable to
  include in the analysis or a string with a filename that contains
  this information.  If the file resides in your working
  directory, only the name of the file is supplied; if it resides
  elsewhere, the complete path must be specified. Setting this
  argument equal to NA does not subset by (i.e., uses all available
  levels of) the geographic nominal index variable.  The code for each
  level to forecast must appear on its own line in the file.  For
  example, if forecasts are only to be constructed for the U.S. and
  the U.K., the file should contain the codes representing these two
  countries, each on its own line.  Default is NA.

\item[skip] An integer; the minimum number of cross-sectional time
  series observations on the dependent variable in the in-sample
  period in order for a level of the geographic nominal index variable
  to be included in the analysis.  This option should only be used
  when the argument \texttt{usercntrylist} is set to NA; setting this
  argument to either NA or $0$ in this case includes all levels in the
  analysis.  For example, if the country Paraguay has only $10$
  observations on the dependent variable (say, for the years
  1980--1989) and \texttt{skip} is equal to $15$, Paraguay will be dropped.
  Default is $8$.
 
\item[cov.select] A string vector; each element of the vector is a
  string that specifies a covariate to exclude from the levels of the
  grouped continuous index variable specified by the argument
  \texttt{age.select}.  If this argument is set to NA, no covariates
  are dropped from the levels specified in \texttt{age.select}.
  Default is NA.
  
\item[age.select] A numeric vector; each element of the vector is a
  level of the grouped continuous index variable for which covariates
  specified in the argument \texttt{cov.select} will be dropped.  If
  this argument is set to NA, no covariates specified in
  \texttt{cov.select} will be dropped.  Default is NA.
  
\item[transform] An integer; how the dependent variable should be
  transformed.  Options are 1 for $\log(x)$; 2 for $\sqrt{x}$; 3 for
  $\log(x+1)$; and NA for no transformation.  Default is 1.
\end{description}

\noindent \emph{Parameters related to forecasting:}

\begin{description}  
\item[model] A string; the method used for forecasting.  Options are:
  Bayes with Gibbs sampling (``Bayes''), Bayes maximum a posteriori,
  without Gibbs sampling (``MAP''), Ordinary Least Squares (``OLS''),
  Lee-Carter (``LC''), and Poisson (``POISSON'').  If the argument is not
  a string, e.g., argument \texttt{model} is equal to NA,
  \texttt{yourcast()} does not produce forecasts; rather, it only
  processes the data.  Advanced users may include their own functions
  by defining ``model'' as a pointer to a function.  Default is
  ``OLS''.
  
\item[fore] An integer; the last year of the out-of-sample period,
  i.e. the last year for which a forecast will be constructed.
  Default is 2030.  See the discussion under the argument \texttt{lag}
  for additional information.
  
\item[yrest] An integer; the last year of the in-sample period, i.e.
  the last year for which actual data on the dependent variable and
  covariates exists.  Default is 2000.  See the discussion under the
  argument \texttt{lag} for additional information.
  
\item[lag] An integer; the lag of the covariates used in forecasting.
  Once the covariates have been lagged, data on the dependent variable
  for a year $t$ will be associated with data on the covariates for
  the year $(t - lag)$.  For example, given a lag of 10, mortality for
  a cross-sectional unit in 1980 will be associated with the
  covariates for that cross-sectional unit in 1970.  Note that
  constraints exist on the lag structure: \texttt{lag} must be greater
  than or equal to $fore - yrest$. If \texttt{lag} is less than $fore
  - yrest$, then for years in the range $(yrest + lag, fore)$, there
  is no covariate data for use in forecasting.  For example, if
  \texttt{lag} is equal to $10$, \texttt{fore} is equal to $2020$, and
  \texttt{yrest} is equal to $2000$, there is no covariate data
  available for the years 2010 to 2020 ($10 < 2020 - 2000 = 20$).
  Additionally, for each cross-sectional unit, let $f_d$ equal the
  first year for which we possess data on the dependent variable;
  $f_c$ equal the first year for which we possess data on the
  covariates; and \texttt{yrest} and \texttt{fore} be as defined
  above.  If \texttt{lag} is greater than $f_d - f_c$, for years in
  the range $(f_d, f_c + lag)$, there is no covariate data to
  associate with data on the dependent variable.  Consequently,
  \texttt{yourcast()} will only use a constant and time as covariates
  for such cross-sectional units (other covariates will be dropped and
  these added, if you did not include them in the argument
  \texttt{covariates}).  For example, if $f_d = 1950$, $f_c = 1920$,
  and \texttt{lag} is equal to $40$, for the years 1950 to $(1920 +
  40) = 1960$, there is no covariate data to associate with mortality
  data ($40 > 1950 - 1920 = 30$).  To make maximal use of all
  available data on the covariates and dependent variable, we
  recommend that the argument \texttt{lag} be set equal to $f_d - f_c$
  and the argument \texttt{fore} be set equal to $yrest + lag$.
  Default is $30$.
 
  Note: Please pay attention if the parameter ``cov'' comprises 
  any covariate of the type dependent-variable.  You may loose 
  part of the actual ``depvar'' cross-sectional time series 
  after lagging all the covariates if the dependent-variable-type covariate  
  contains some missing values at the start of the time series. 
  
\item[svdtol] A scalar; the tolerance used in inverting a matrix by
  SVD.  Default is $10^{-10}$.
\end{description}

\noindent \emph{Parameters related to data input and output:}

\begin{description}
\item[save.output] A boolean or a string; if a boolean, it specifies
  whether the \texttt{yourcast()} output object should be saved to a
  file. If TRUE, the output object will be saved to the file
  \texttt{depvar\_strata\_model.dat}. If a string, the name of the
  file to which the \texttt{yourcast()} output object will be saved.
  Note that \texttt{yourcast()} will automatically set this argument to
  FALSE if the argument \texttt{model} is set to NA.  Default is TRUE.
  
\item[reuse.data] A boolean; should \texttt{yourcast()} reuse the
  pre-processed data file produced by a prior call to
  \texttt{yourcast()}?  If the argument is FALSE, data pre-processing
  occurs.  If the argument is TRUE, \texttt{yourcast()} will attempt to
  load a preprocessed data file.  If either this file does not exist
  or existing arguments saved in it do not match the current arguments
  supplied to \texttt{yourcast()}, data pre-processing occurs;
  otherwise, the pre-processed data file is loaded.  Default is TRUE.

\item[save.FULL] A boolean; should \texttt{yourcast()} save the expanded
  versions of the data files for covariates of type strata.age.independent, 
  age.independent, and strata.independent in the same directory where the
  raw covariate data files are stored (i.e., in the directory given
  by \texttt{cov.path})?  If so, these files will have names of form
  \texttt{FULL.``cov''.txt}, where ``cov'' is the name of the covariate, and  
  three columns, although the third column will be filled
  with NA's for strata.age.independent and strata.independent
  covariates.  They will run over all groups relevant to the covariate
  type, e.g. all grouped continuous-geographic nominal groups for
  strata.age.independent and strata.independent covariates and over
  all grouped continuous-geographic nominal-strata groups for 
  age.independent covariates.  Once expanded files have been saved to 
  the directory \texttt{cov.path}, the argument \texttt{cov.type} may 
  be set to NA even though covariates are not all dependent upon both 
  the grouped continuous variable and the strata.  In this case, 
  continue to supply covariates to the argument \texttt{cov} as before 
  (not, for example, as ``FULL.cov'', where ``cov'' is the name of the 
  covariate).  Default is FALSE.
  
\item[reuse.path] A string; the name of the directory in which to look
  for the pre-processed data file when the argument
  \texttt{reuse.data} equals TRUE.  Usually this argument is identical
  to the argument \texttt{out.path}.  Either a sub-directory of the
  working directory or a complete path.  Default is ``OUTPUT/''.
  
\item[prior.path] A string; the name of the directory where precomputed
  quantities related to the prior, e.g. the mean profile, are stored.
  Either a sub-directory of the working directory or a complete path.
  Default is ``DATA/''.
  
\item[data.path] A string; the directory where the dependent variable
  data is stored.  Either a sub-directory of the working directory or
  a complete path.  Default is ``DATA/''.
  
\item[cov.path] A string; the directory where the covariates are
  stored.  Usually the same as the argument \texttt{data.path}.
  Either a sub-directory of the working directory or a complete path.
  Default is ``DATA/''.
  
\item[out.path] A string; the directory where the output and processed
  data files will be saved.  Either a sub-directory of the working
  directory or a complete path.  \texttt{yourcast()} will automatically
  create this directory if it does not already exist.  Default is 
  ``OUTPUT/''.

\item[codes.names]  A string; the name of a file that associates 
  the names of the levels of the geographic nominal variable with
  the codes, used to generate output more amenable to graphing.  If
  this file does not exist, codes will be used in graphing instead
  of names; in this case, the argument should be set equal to NA.
  The file has two columns, code and name; is white-space delimited;
  and contains one level of the geographic nominal variable per line.
  It resides in the directory specified by the argument 
  \texttt{data.path}.  Default is NA.
\end{description}

\noindent \emph{Parameters related to data processing:}

\begin{description}
\item[lag.cutoff] A scalar; the percentage of data within a given
  cross-section that can be lost in order to include covariates with
  missing values.  If the percentage of lost data exceeds the value of
  the argument, covariates with missing values are eliminated from the
  cross-section (if all covariates are eliminated in this manner, an
  error message is returned).  For example, if $10$\% of the
  observations from a given cross-section contain missing values on at
  least one covariate and \texttt{lag.cutoff} is equal to $0.08$,
  covariates with missing values are dropped from that cross-section;
  however, if \texttt{lag.cutoff} is equal to $0.20$, no covariates are
  dropped.  Default is $1$.
  
\item[elim.collinear] A boolean; should collinearity in covariates be
  searched for and, if necessary, collinear covariates deleted during
  pre-processing?  Note that if the argument is TRUE,
  \texttt{yourcast()} imposes the constraint that the covariates must
  include a constant term (i.e., ``cnst'' must be an element of the
  argument \texttt{covariates}).  Default is TRUE.
  
\item[tol] A scalar; the tolerance for detecting collinearity in
  covariates for a particular cross-sectional unit.  This argument
  must take values between $0$ and $1$, inclusive.  If the argument takes
  the value $1$, it has no effect. Use in conjunction with ``delta.tol''
  and ``solve.tol'' in the process of eliminating collinear covariates. 
  Default is $0.9999$.

\item[delta.tol] A real number smaller than $1$ that decreases the value 
  of ``tol'' in the process of eliminating collinearities.  
  (see description for ``tol''). Default is $0.005$. 

\item[solve.tol] A real number smaller than $1$ that is used in the argument of 
  the R-function ``solve'' to invert matrices (see description for \texttt{tol}).  
  Default is $1.e-10$.   

\item[standardize] A boolean; should the covariates in each
  cross-sectional unit be standardized (to zero mean and standard
  deviation of $1$)?  Standardization is performed for both the in- and
  out-of-sample periods.  Default is FALSE.
\end{description}

\noindent \emph{Parameters governing the CSTS identification code structure:}

\begin{description}
\item[cntry.digits] A scalar; the number of digits used to identify
  the level of a generic cross-sectional index variable with elements
  arranged geographically (i.e., not formed as a grouped continuous
  variable).  This is usually used to code country or state.  Default
  is 4.  (For example, the WHO mortality data sets use a four-digit
  code to identify countries, e.g. USA = 2450.)
  
\item[year.digits] A scalar; the number of digits of the variable that
  defines the time series component of the cross-section time series
  data structure.  Default is 4. (For example, the WHO mortality data
  sets use a four-digit code to identify the year, e.g.\ 1980.)
  
\item[age.digits] A scalar; the number of digits used to identify the
  level of a grouped continuous cross-sectional index variable.  For
  example, the WHO mortality data sets use a two-digit code to
  identify an age group, e.g.\ ages 20--24 = 20.  Default is 2.
  
\item[digit.first] A scalar; the number of digits into the string of
  the cross-section time series identification code that the code
  actually begins; in other words, the number of initial digits to
  discard.  This allows you to add a label not used by the program.
  Default is 0.
\end{description}

\noindent \emph{Parameters specific to the Girosi-King Forecasting Method:}

\begin{description}
\item[zero.mean] A boolean or a string; this option is specific to the
  Girosi-King forecasting method. If a boolean, it specifies whether
  the prior on the expected value of the dependent variable has zero
  mean. If FALSE, the prior is not zero mean and is centered around
  the mean profile of the grouped continuous index variable:  the
  average over time and levels of the geographic nominal variable for
  each level of the grouped continuous index variable.
  \texttt{yourcast()} will automatically generate this quantity.  A
  non-zero mean is achieved by subtracting this mean from the
  dependent variable and then adding it back after the forecast has
  been done.  If TRUE, no action is taken. If string, this is the
  name of a file that contains the mean profile to be used in 
  constructing a non-zero mean prior. The file resides in the 
  directory \texttt{prior.path}.  Default is FALSE.

\item[Ha.deriv] A numeric vector; used when smoothing the level of the
  expected value of the dependent variable over the grouped continuous
  index variable with the Girosi-King forecasting method.  The weight
  of each derivative taken with respect to the grouped continuous
  index variable in the smoothness functional for the prior.  This
  argument controls the degree of smoothness (the higher the order of
  derivative, the more local smoothness) and the form of prior
  indifference.  If using a mixed smoothness functional to separately
  control these characteristics, the highest order derivative controls
  the former and the lowest order the latter.  The first element of
  the vector corresponds to the weight on the derivative of order 0
  (the identity operator), the second to the weight on the derivative
  of order 1 (the 1st derivative), and the third to the weight on the
  derivative of order 2 (the 2nd derivative), etc.  For example, c(0,
  1, 1) corresponds to a mixed functional that penalizes the first and
  second derivatives equally and c(0, 0, 1) to a standard functional
  that penalizes the second derivative.  Usually a second order
  derivative works well.  Default is c(0, 0, 1).
  
\item[Ha.age.weight] A scalar or a numeric vector; used when smoothing
  the level of the expected value of the dependent variable over the
  grouped continuous index variable with the Girosi-King forecasting
  method.  The weight given to each level of the grouped continuous
  variable in the smoothness functional for the prior.  This argument
  controls whether a varying degree of smoothness is allowed for
  different levels of the grouped continuous variable or or all levels
  are penalized equally for lack of smoothness.  If the argument takes
  the values 0 or NA, levels are weighed equally; if the argument
  takes a scalar value, the weight of level a is proportional to
  $a^{Ha.age.weight}$; if the argument is a vector of length A,
  the element of the vector is taken as the weight of level a.
  Default is 0.
  
\item[Ha.time.weight] A scalar or a numeric vector; used when
  smoothing the level of the expected value of the dependent variable
  over the grouped continuous index variable with the Girosi-King
  forecasting method.  The weight given to each time period in the
  smoothness functional for the prior. Similar to Ha.age.weight; is
  the average lack of smoothness over the levels of the grouped
  continuous variable penalized equally in every time period or are
  certain time periods penalized more heavily than others?  See the
  discussion under the argument \texttt{Ha.age.weight} for the values
  that this argument can take.  Default is 0.
  
\item[Ha.sigma] A scalar; indicates whether or not the level of the
  expected value of the dependent variable is smoothed over the
  grouped continuous index variable with the Girosi-King forecasting
  method.  If NA, it is not.  If a scalar, it is.  In the latter case,
  the argument is the average standard deviation of the expected
  values of the dependent variable over the levels of the grouped
  continuous index variable desired for the prior.  A larger standard
  deviation represents more uncertainty about the knowledge embodied
  in the prior, which allows the data to play a greater role in model
  estimation.  Default is 0.30.

\item[Ha.sigma.sd] A scalar; the standard deviation of the parameter 
  \texttt{Ha.sigma}, used to smooth over the
  grouped continuous index variable with the Girosi-King forecasting
  method and which yields the priors for the model. Default is 0.1. 
   
\item[Ht.deriv] A numeric vector; used when smoothing the level of the
  expected value of the dependent variable over the time index
  variable with the Girosi-King forecasting method.  The weight of
  each derivative taken with respect to the time index variable in the
  smoothness functional for the prior.  See the discussion under the
  argument \texttt{Ha.deriv} for further information.  Default is c(0,
  0, 1).
  
\item[Ht.age.weight] A scalar or a numeric vector; used when smoothing
  the level of the expected value of the dependent variable over the
  time index variable with the Girosi-King forecasting method.  The
  weight given to each level of the grouped continuous variable in the
  smoothness functional for the prior.  See the discussion under the
  argument \texttt{Ha.age.weight} for further information.  Default is
  0.
  
\item[Ht.time.weight] A scalar or a numeric vector; used when
  smoothing the level of the expected value of the dependent variable
  over the time index variable with the Girosi-King forecasting
  method.  The weight given to each time period in the smoothness
  functional for the prior. See the discussion under the argument
  \texttt{Ha.time.weight} for further information.  Default is 0.
  
\item[Ht.sigma] A scalar; indicates whether or not the level of the
  expected value of the dependent variable is smoothed over the time
  index variable with the Girosi-King forecasting method.  If NA, it
  is not.  If a scalar, it is.  In the latter case, the argument is
  the average standard deviation of the expected values of the
  dependent variable over the levels of the time index variable
  desired for the prior.  See the discussion under the argument
  \texttt{Ha.sigma} for more information.  Default is 0.3.

\item[Ht.sigma.sd] A scalar; the standard deviation of the parameter 
  \texttt{Ht.sigma}, used to smooth over time 
  for  the Girosi-King forecasting
  method and which yields the priors for the model. Default is 0.1.  
  
\item[Hat.a.deriv] A numeric vector; used when smoothing the time
  trend of the expected value of the dependent variable over the
  levels of the grouped continuous index variable with the Girosi-King
  forecasting method.  The weight of the mixed second- and
  higher-order partial derivatives, taken first with respect to time
  and then with respect to the grouped continuous index variable, in
  the smoothness functional for the prior.  See the discussion under
  the argument \texttt{Ha.deriv} for further information.  Default is
  c(0, 0, 1).
  
\item[Hat.t.deriv] A numeric vector; used when smoothing the time
  trend of the expected value of the dependent variable over the
  levels of the grouped continuous index variable with the Girosi-King
  forecasting method.  The weight of the partial derivative taken with
  respect to time in the smoothness functional for the prior.  The use
  of the first or a higher order partial derivative in a standard
  functional is supported.  Default is c(0, 1).
  
\item[Hat.age.weight] A scalar or a numeric vector; used when
  smoothing the time trend of the expected value of the dependent
  variable over the levels of the grouped continuous index variable
  with the Girosi-King forecasting method.  The weight given to each
  level of the grouped continuous variable in the smoothness
  functional for the prior.  See the discussion under the argument
  \texttt{Ha.age.weight} for further information. Default is 0.
  
\item[Hat.time.weight] A scalar or a numeric vector; used when
  smoothing the time trend of the expected value of the dependent
  variable over the levels of the grouped continuous index variable
  with the Girosi-King forecasting method.  The weight given to each
  time period in the smoothness functional for the prior. See the
  discussion under the argument \texttt{Ha.time.weight} for further
  information. Default is 0.
  
\item[Hat.sigma] A scalar; indicates whether or not the time trend of
  the dependent variable is smoothed over the levels of the grouped
  continuous index variable with the Girosi-King forecasting method.
  If NA, it is not.  If a scalar, it is.  In the latter case, the
  argument is the average standard deviation of the expected values of
  the dependent variable over the levels of the grouped continuous
  index variable desired for the prior.  See the discussion under the
  argument \texttt{Ha.sigma} for more information.  Default is 0.2.

\item[Hat.sigma.sd] A scalar; the standard deviation of the parameter 
  \texttt{Ht.sigma}, used to smooth over time trends and the expected 
  value of the levels of the grouped continuous index variable
  with the Girosi-King forecasting method and  
  which yields the priors for the model. Default is 0.1.  
   
\item[Hct.c.deriv] A string or NA; the name of a file containing 
  the adjacency matrix to be used in smoothing over the geographic 
  nominal index variable with the Girosi-King forecasting method. 
  If you only supply a file name, the file must reside in the 
  directory specified by the argument \texttt{prior.path}.
  If the file resides elsewhere, the complete
  path must be specified.  If NA, no file is supplied.   In this case, if
  \texttt{Hct.sigma} is set to a number, the simulation will stop and 
  a warning will be issued that you either need to specify 
  the name of the file or set \texttt{Hct.sigma} equal to NA to signal
  to \texttt{yourcast} that smoothing should not take place over the levels
  of the geographic nominal index variable.  Default is ``adjacency.txt''.
  
\item[Hct.t.deriv] A numeric vector; controls whether smoothing of the
  expected value \textit{or} smoothing of the time trend of the expected value
  of the dependent variable takes place over the levels of the
  geographic nominal index variable with the Girosi-King forecasting
  method.  Both cannot be done simultaneously.  To smooth the level of
  the expected value of the dependent variable over the 
  geographic nominal index variable, set the argument equal to 1, the
  identity.  To smooth the time trend, set the argument as in
  \texttt{Hat.t.deriv} so that it serves as the weight of the partial
  derivative taken with respect to time in the standard smoothness
  functional for the prior.  The use of the first or higher order
  partial derivatives is supported.  Default is 1.
    
%%\item[Hct.cntry.weight] A scalar or a numeric vector; used when
%%  smoothing the level or the time trend of the expected value of the
%%  dependent variable over the geographic nominal index variable with
%%  the Girosi-King forecasting method.  The weight given to each level
%%  of the geographic nominal variable in the smoothness functional for
%%  the prior.  See the discussion under the argument
%%  \texttt{Ha.age.weight} for further information.  Default is 0.

%%  The above parameter is currently not used by yourcast:  weights
%%  are only allowed to be placed on time periods.
%%  EV: Federico decided to get away with this parameter
  
\item[Hct.time.weight] A scalar or a numeric vector; used when
  smoothing the level or the time trend of the expected value of the
  dependent variable over the geographic nominal index variable with
  the Girosi-King forecasting method.  The weight given to each time
  period in the smoothness functional for the prior. See the
  discussion under the argument \texttt{Ha.time.weight} for further
  information.  Default is 0.
  
\item[Hct.sigma] A scalar or NA; indicates whether or not the level of the
  expected value of the dependent variable is smoothed over the
  geographic nominal index variable with the Girosi-King forecasting
  method.  If NA, it is not.  If a scalar, it is.  In the latter case,
  the argument is the average standard deviation of the expected
  values of the dependent variable over the levels of the geographic
  nominal index variable desired for the prior.  See the discussion
  under the argument \texttt{Ha.sigma} for more information.  Note
  that the argument \texttt{Hct.c.deriv} may take precedence over this
  argument. On the one hand, if this argument is a number but 
  \texttt{Hct.c.deriv} is set equal to NA,  
  \texttt{yourcast()} stops the simulation and advises you to either supply 
  the name of the file or to set \texttt{Hct.sigma} equal to NA.
  On the other hand, if you supply a file name to \texttt{Hct.c.deriv} 
  but \texttt{Hct.sigma} equals NA, then no smoothing over the geographic nominal 
  index variable takes place.  Default is 0.3.

\item[Hct.sigma.sd] A scalar; the standard deviation of the parameter 
  \texttt{Hct.sigma}, used to smooth over time trends and the geographic nominal 
  index variable with the Girosi-King forecasting
  method and which yields the priors for the model. Default is 0.1.
 
\item[LI.sigma.mean] A scalar; also used in the calculation of the priors 
  in conjunction with \texttt{Ha.sigma.sd}, \texttt{Hat.sigma.sd}, 
  \texttt{Ht.sigma.sd}, and \texttt{Hct.sigma.sd}.
  Default is 0.2.

\item[LI.sigma.sd] A scalar; the standard deviation of \texttt{LI.sigma.mean}
  used in the calculation of the priors. Default is 0.1.
 
\item[nsample] A scalar;  represents the number of iterations of the 
  Gibbs algorithm for the Girosi-King forecasting model ``BAYES''.  
  Default is 500.  
\end{description}

\subsubsection{Value}

The value of the function \texttt{yourcast()} varies as follows.

\begin{enumerate}
 
\item Processed data objects and your related input parameters are
  saved in a processed data file to your specified path.  These
  objects include lists of in- and out-of-sample data on both the
  dependent variable and the covariates for each cross-sectional unit;
  lists of the covariates included in the analysis for each
  cross-sectional unit; and lists of the levels of the geographic
  nominal and grouped continuous variables included in the analysis.
  The file name will have the format
  \texttt{data\_``depvar''\_``strata''.dat}, where ``depvar''
  and ``strata'' are the values of the corresponding
  arguments you supply to \texttt{yourcast()}, e.g.
  \texttt{data\_brst\_2.dat}.  If there are no strata (i.e.,
  \texttt{strata} = NA), the file name will have the format
  \texttt{data\_``depvar''\_NA.dat}.  

  The objects in the processed data file are stored with internal code
  names, which may be different from the input and output names of
  \texttt{yourcast()}.  If you wish to load the preprocessed data file
  into your working environment and to retrieve some of the data
  objects stored in the file, you must use the R-function
  \texttt{load.data} that we supply as part of \YourCast.  This
  function takes the argument \texttt{file}, the processed data file
  name that you must provide, and a number of optional arguments.  In
  its simplest form you type
  \begin{verbatim}   
       > load.data(file = ``data\_cvds\_3.dat'') 
  \end{verbatim}    
  at the command prompt, which loads into your working environment the 
  objects generated in the preprocessing phase of the simulation.  
  See section \ref{sec:helper} for further details about this function.  

  The objects contained in the preprocessed data file are listed below.  
  \begin{description}
  \item[cntry.vec] A numeric vector containing the levels of the
    geographic nominal index variable that were ultimately included in
    the analysis.
  \item[cntry.names.lst] A vector, the names and values of the elements
    of which are the numeric codes representing the levels of the
    geographic nominal index variable included in the analysis and the 
    strings of their names, respectively.  (Note that in the output 
    object and file we refer to this parameter as ``cntry.lst''.)   
  \item[age.vec] A numeric vector containing the levels of the grouped
    continuous index variable that were ultimately included in the
    analysis.
  \item[inpopul] If a second data input file is supplied to generate a
    ``rate'' dependent variable, signaled by your setting the parameter 
    \texttt{population} equal to a string, a list.   
    Each element is a $t$-by-1 matrix of the in-sample data 
    on the population data for a cross-sectional unit.  If a second
    data input file is not supplied, a list of $1$s.
  \item[insampy] A list, each element of which is a $t$-by-1 matrix of
    the in-sample data on the dependent variable for a cross-sectional
    unit; $t$ is the number of in-sample years in the time series for
    that cross-section.
  \item[insampid] A list, each element of which is a $t$-by-1 matrix
    containing the in-sample CSTS identification code; $t$ is as before.
  \item[insampx] A list, each element of which is a $t$-by-$k$ matrix of
    the in-sample covariate data for a cross-sectional unit; $t$ is as
    before and $k$ is the number of covariates for that cross-section.
  \item[outpopul]  If a second data input file is supplied (see discussion
    under \texttt{inpopul}), a list. 
    Each element is a $t$-by-1 matrix of the out-of-sample data 
    on the population variable for a cross-sectional unit.  If a second
    data input file is not supplied, a list of $1$s.
  \item[outsampy] A list, each element of which is a $t$-by-1 matrix of
    the out-of-sample data on the dependent variable for a
    cross-sectional unit; $t$ is the number of out-of-sample years in
    the time series for that cross-section.
  \item[outsampid] A list, each element of which is a $t$-by-1 matrix
    containing the out-of-sample CSTS identification code; $t$ is as
    before.
  \item[outsampx] A list, each element of which is a $t$-by-$k$ matrix of
    the out-of-sample covariate data for a cross-sectional unit; $t$ is
    as before and $k$ is the number of covariates for that
    cross-section.
  \item[covlist] A list, each element of which is a $t$-by-$k$ matrix
    containing the in- and out-of-sample covariates for a
    cross-sectional unit; $t$ is the length of the time series for that
    cross-section.
  \item[covid] A list, each element of which is a $t$-by-1 matrix
    containing the CSTS unit identification code for a cross-section;
    $t$ is the length of the time series for that cross-section. 
  \item[list.covariates] A list, each element of which is a string
    vector that contains the names of the covariates ultimately used
    for a cross-sectional unit.
  \item[parameters] A list, each element of which is the value you supplied
    for a parameter relevant to data processing, i.e.\ \texttt{depvar}, 
    \texttt{strata}, \texttt{cov}, \texttt{cov.type}, \texttt{fore}, 
    \texttt{yrest}, \texttt{lag}, \texttt{usercntrylist}, 
    \texttt{userages}, \texttt{skip}, \texttt{standardize}, 
    \texttt{transform}, \texttt{data.path}, and \texttt{cov.path}.
  \end{description}
  
\item If forecasting is performed, the forecast object is returned 
  invisibly as a six-element list.  This object contains the
  in- and out-of-sample predictions, the actual in- and out-of-sample
  data on the dependent variable, and the model parameters for each
  level of the geographic nominal index variable.  Data is organized 
  by level of the geographic nominal index variable to facilitate 
  graphing.  The elements of this list appear below.
  \begin{description}
  \item[yhatin] A list, each element of which is a $t$-by-$a$ matrix
    containing the predicted values of the dependent variable over the
    in-sample period for a level of the geographic nominal index
    variable; $t$ is the length of the time series for that level and $a$
    is the number of levels of the grouped continuous index variable.
    Each matrix is tagged with the code representing the appropriate
    level of the geographic nominal index variable.
  \item[yhatout] A list, each element of which is a $t$-by-$a$ matrix
    containing the predicted values of the dependent variable over the
    out-of-sample period for a level of the geographic nominal index
    variable; $t$, $a$, and tagging are as before.
  \item[insampy] A list, each element of which is a $t$-by-$a$ matrix
    containing the actual values of the dependent variable over the
    in-sample period for a level of the geographic nominal index
    variable; $t$, $a$, and tagging are as before.
  \item[outsampy] A list, each element of which is a $t$-by-$a$ matrix
    containing the actual values of the dependent variable over the
    out-of-sample period for a level of the geographic nominal index
    variable; $t$, $a$, and tagging are as before.
  \item[coeff] A list, each element of which is either a vector or a
    list of vectors containing the estimated model parameters, if any,
    for a cross-sectional unit.  The tag of each is the
    cross-sectional unit code.
  \item[std] A list, each element of which is either a vector or a
    list of vectors containing the estimated standard error of the
    coefficients or the estimated standard deviation, if any, for a
    cross-sectional unit.  Tagging is as before.
  \item[parameters] A list, each element of which is the value you
    supplied to a parameter of \texttt{yourcast()}:
    \texttt{depvar}, \texttt{strata}, \texttt{yrest}, \texttt{model},
    \texttt{age.vec}, and \texttt{cntry.lst} (same as the object
    \texttt{cntry.names.lst} from the data processing file). 
  \end{description}
  
  Additionally, if and only if the argument \texttt{save.output} is
  equal to TRUE, the forecast object along with most globals; a list
  of the covariates used in each cross-section; and lists of the
  levels of the geographic nominal index and the grouped continuous
  variables included in the analysis are saved in an output file to
  the path you specified via the parameter \texttt{out.path}. The name
  of the file has format
  \texttt{``depvar''\_``strata''\_``model''.dat}, where ``depvar'',
  ``strata'', and ``model'' are the corresponding values of the
  arguments you supplied to \texttt{yourcast()},
  e.g. \texttt{brst\_2\_ols.dat}.  If there are no strata (i.e.,
  \texttt{strata} = NA), the file name will have the format
  \texttt{``depvar''\_NA\_``model''.dat}.  The output file contains
  the same objects as the list returned by \texttt{yourcast()}.  A
  description of this list, and thus of the objects contained in the
  output file, can be found above.  You may load the output file into
  your working environment with the actual names used in the
  simulation by using the basic R-function \texttt{load}.  Please
  consult the R-manual or type \texttt{help(load)} to obtain
  information about this function.
\end{enumerate}

\subsection{Function `yourgraph'}\label{sec:refgraph}

This function produces the summary graphics shown or discussed in
\cite{GirKin04}.  It takes as its argument an object of class
\texttt{yourcast.obj}, the output of a call to \texttt{yourcast()},
or a similarly-formatted object.

\subsubsection{Usage}

\begin{verbatim}
  yourgraph(input.data, obs.insample = TRUE, obs.outsample = TRUE,
  pred.insample = TRUE, pred.outsample = TRUE, plot.time.series = TRUE, 
  plot.age.profiles = TRUE, wide = 8, high = 8, page.horizontal = 
  FALSE, row.num = 1, col.num = 2, background.col = ``tan1'', 
  print2file = TRUE, dv.transform = function(x){x}, strata.label = NULL,
  age.label = NULL, out.path = NULL)
\end{verbatim}

%% HMS:
%% (1)  Eliminated parameter yourcast.  Unless I've missed something, 
%%      this parameter is not used by the function.
%% (2)  Added parameter out.path to allow users to specify a location
%%      in which to save the graphical file.  Also added that the default
%%      location (if no value is supplied via this parameter) is that
%%      specified by parameter out.path to yourcast.
%% (3)  Added parameter strata.label to allow users to specify a label
%%      for the strata.
%% (4)  Added parameter age.label to allow users to specify a label
%%      for the grouped continuous index variable.
%% (5)  The function currently relies on long.causes to translate
%%      depvar into long disease name.  This needs to be modified for
%%      non-mortality forecasting.  Federico suggests using as the default
%%      (if depvar does not match any of the causes of mortality in long.
%%      causes) the file name of the dependent variable.  He also suggests
%%      allowing advanced users to pass an R function of their own that
%%      translates depvar into graph titles.  I've not currently written
%%      up this last option, but it certainly could be done.

\subsubsection{Inputs}

\begin{description}
  \item[input.data] A string; the name of the object containing the
    data to be summarized graphically, usually the output from
    \texttt{yourcast()}.  This can either be the output file,
    e.g. \texttt{allc\_2\_ols.dat}, or the returned object from a
    command such as \texttt{my.forecast <- yourcast()}.  Default is
    NULL.
  \item[obs.insample] A Boolean; should the observed data for the
    in-sample period be plotted?  If TRUE, observed, in-sample data is
    plotted in all of the graphs that you choose to generate, as
    signaled to \texttt{yourgraph} by the values of the parameters
    \texttt{plot.time.series} and \texttt{plot.age.profiles}.  Default
    is TRUE.
  \item[obs.outsample] A Boolean; should the observed data for the
    out-of-sample period be plotted?  If TRUE, observed, out-of-sample
    data is plotted in all of the graphs that you choose to generate,
    as signaled to \texttt{yourgraph} by the values of the parameters
    \texttt{plot.time.series} and \texttt{plot.age.profiles}.  Default
    is TRUE.
  \item[pred.insample] A Boolean; should the forecasts for the
    in-sample period be plotted?  If TRUE, forecasted, in-sample data is
    plotted in all of the graphs that you choose to generate, as
    signaled to \texttt{yourgraph} by the values of the parameters
    \texttt{plot.time.series} and \texttt{plot.age.profiles}.  Default
    is TRUE.
  \item[pred.outsample] A Boolean; should the forecasts for the
    out-of-sample period be plotted?  If TRUE, forecasted, out-of-sample
    data is plotted in all of the graphs that you choose to generate,
    as signaled to \texttt{yourgraph} by the values of the parameters
    \texttt{plot.time.series} and \texttt{plot.age.profiles}.  Default
    is TRUE.
  \item[plot.time.series] A Boolean; should a graph plotting the data
    for one level of the geographic nominal index variable against the
    levels of the time series index variable be generated?  The graph
    contains one line for each level of the grouped continuous index
    variable, and a separate graph is generated for each level of the
    geographic nominal index variable.  In the case of forecasting
    mortality, this graph displays the time series for a country--age
    unit, for all such units for a given country.  Default is TRUE.
  \item[plot.age.profiles] A Boolean; should a graph plotting the data
    for one level of the geographic nominal index variable against the
    levels of the grouped continuous index variable be generated?  The
    graph contains one line for each level of the time series index
    variable, and a separate graph is generated for each level of the
    geographic nominal index variable.  In the case of forecasting
    mortality, this graph displays the age profiles over time for a
    country.  Default is TRUE.
  \item[wide] A scalar; the width in inches of the graphics region in
    the postscript file.  Default is $8$.
  \item[high] A scalar; the height in inches of the graphics region in
    the postscript file.  Default is $8$.
  \item[page.horizontal] A Boolean; should the orientation of the
    image in the postscript file be horizontal?  Default is FALSE.
  \item[row.num] A scalar; the number of rows into which the graphs
    should be organized.  Graphs are printed first by level of the
    geographic nominal index variable and then by type, with the time
    series graph preceding the grouped continuous index variable
    profile graph.  Following \texttt{R} convention, printing is by
    row.  Thus, if you choose to generate both the time series and
    grouped continuous index variable profile graphs, one row, and two
    columns (the default values of the relevant parameters), each page
    will contain a 1-by-2 arrangement of the two graphs for a level of
    the geographic nominal index variable.  The time series graph
    will appear in position $(1, 1)$ and the grouped continuous index
    variable profile graph will appear in position $(1, 2)$.  Default is
    $1$.
  \item[col.num] A scalar; the number of columns into which the graphs
    should be organized.  See the discussion under \texttt{row.num}
    for further information.  Default is $2$.
  \item[background.col] A string; the background color for the
    postscript file.  Default is ``tan1''.
  \item[print2file] A Boolean; should output should be printed to an
    encapsulated postscript file instead of to the screen?  Default is TRUE.
  \item[dv.transform] A function; the transformation to be performed
    on the data prior to graphing.  Default is the identity function,
    function(x){x}.
  \item[strata.label] A string; an optional label for the strata that
    is being graphed, if any.  This label both appears on the graphs
    and in the name of the file, if you chose to save the file instead
    of simply printing to the screen.  Should you not supply a label 
    via this argument, \texttt{yourgraph} uses the string 
    ``m'' as the label if the parameter \texttt{strata} from \texttt{yourcast} is
    equal to $2$; ``f'' if the parameter \texttt{strata} is another numeric 
    value; and ``NA'' otherwise.  Default is NULL.
  \item[age.label] A string; an optional label for the horizontal axis 
    of the grouped continuous variable profile graph, which you generate
    by setting the parameter \texttt{plot.age.profiles} equal to TRUE.
    The natural value of this argument is the name of the grouped
    continuous index variable.  Should you not supply a label via this
    argument, \texttt{yourgraph} uses the string ``Age'' to label the
    graph.  This is the name of the grouped continuous index variable
    when forecasting mortality.  Default is NULL.
  \item[out.path]  A string; the name of the directory in which the graphical
    file should be saved.  Either the complete path or a sub-directory of
    your working directory.  Should you not supply a path
    via this argument, \texttt{yourcast} saves the file to the directory
    specified by the parameter \texttt{out.path} to \texttt{yourcast}.
    By default, this is the sub-directory ``/OUTPUT'' of your working
    directory, which will be created if it does not already exist.
    Default is NULL.
\end{description}

\subsubsection{Value}

The output of \texttt{yourgraph()} is a graphical file printed either to
the screen or to an encapsulated postscript (EPS) file, which you choose 
by setting the value of the parameter \texttt{print2file}.  If this 
parameter is set equal to TRUE, graphs are printed to an EPS file.  This 
file is saved in the directory specified by the parameter \texttt{out.path}
to \texttt{yourcast} unless you specify otherwise via the parameter
\texttt{out.path} to \texttt{yourgraph}.  By default, the file is saved
in the sub-directory ``/OUTPUT'' of your working directory, 
which is created if it does not already exist.  The file has name of form
\texttt{trends\_``depvar''\_``gender.str''\_model``model''.eps}, where
``depvar'' and ``model'' are the arguments you supply to
\texttt{yourcast} and ``gender.str'' is a label for the strata. For example, 
one possible file name is \texttt{trends\_allc\_m\_modelMAP.eps}.  
If there is no strata, ``gender.str'' will be equal to ``NA''.

\section{Helper Functions}\label{sec:helper}
\YourCast\ contains some independent helper functions that may be
called from your working (or global) environment.  These helper
functions are useful to process and manipulate the data as well as to
gain additional insight into and information about the simulation.  We
enumerate and explain how to use them from the global environment.

\subsection{Retrieving the Arguments of \texttt{yourcast()}: 
\texttt{args.default}}

The function \texttt{args.default()} returns a list, the
elements of which have the names of the input parameters to
\texttt{yourcast()} described in section \ref{sec:refyour} and contain
the default values of these parameters. To access the names of the
parameters returned by the function, type
``\texttt{names(args.default())}'' at the R prompt.  Their value may
be obtained in the usual way of retrieving elements from a list, such
as
\begin{verbatim}
     > res <- args.default()
     > res$model 
       OLS
     > res$strata
       3
\end{verbatim}%$ 
To get a complete listing of all parameters and their default values,
simply type  
\begin{verbatim}
     > args.default()
\end{verbatim} 
at the command prompt.

\subsection{Loading the Preprocessed Data File: \texttt{load.data}}

Processed data files, such as \texttt{data\_depvar\_strata.dat}, may
be loaded in your working environment using the function
\texttt{load.data()}, which is a wrapper for the basic R-function
\texttt{load()}.  The function has only one required argument,
\texttt{file}.  If the processed data file resides in the
sub-directory ``OUTPUT/'' of your working directory, you only need to
specify the file name using the argument \texttt{file}. If it resides
somewhere else, then you need to supply the complete path as a string
using the argument \texttt{output.path} as well as the file name using
the argument \texttt{file}.  All of the other arguments to
\texttt{load.data()} are optional.  You may use them to change the
default names of the objects from the file that will be loaded into
the global environment.  The default names are obtained with the
command
\begin{verbatim}
     > names(formals(load.data))
       ``file''            ``output.path''     ``depvar''   
       ``cov''             ``fore''            ``yrest''    
       ``population''      ``usercntrylist''   ``userages'' 
       ``standardize''     ``transform''       ``data.path''
       ``covid''           ``insampy''         ``insampid'' 
       ``insampx''         ``outsampy''        ``outsampid''       
       ``outsampx''        ``cntry.vec''       ``age.vec'' 
       ``cntry.names.lst"  ``strata''          ``lag''
       ``skip''            ``covlist''         ``inpopul''
       ``outpopul''        ``list.covariates''
\end{verbatim}
However, you may change any of these names.  For example, you might
want to rename \texttt{strata} as ``gender'' and \texttt{insampy},
which is a list containing the cross-sectional time series data on the
dependent variable for the in-sample period, as ``mortality''.  To do
so, type the following at the command prompt:
\begin{verbatim}
     > res <- load.data(file = ``data_allc_3.dat'', strata = 
     + ``gender'', insampy = ``mortality'')
\end{verbatim}
This command will put all of the objects from the processed data file
listed above in your working environment under their default names with
the exception of ``strata'' and ``insampy'', which will be renamed as
``gender'' and ``mortality''.  If you want to see the names of the
objects you are loading, the object returned by \texttt{load.data()}
is a list that contains the default names of the objects stored in the
file and loaded into your workspace, as well as the new names that you
have assigned to them.  In the example above, we named this list
``res''.  A different directory path is supplied as follows:
\begin{verbatim}
     > load.data(file = ``data\_cvds\_3.dat'', output.path = 
     + ``C:/mydirectory/'')
\end{verbatim}
This command puts all objects in your global environment under
their default names and looks for the processed data file
in the directory \texttt{C:/mydirectory}.

\subsection{Choosing Relevant Age Groups: \texttt{drop.ages}}

This function is only relevant when forecasting mortality using the
WHO data sets available from \url{http://gking.harvard.edu}.  Under
these circumstances, each possible dependent variable---a particular
cause of mortality---is associated with some age groups for which the
mortality rates are relevant.  For example, breast cancer---one cause
of mortality that you might choose to forecast---does not strike the
very young, so the relevant age groups for which forecasts should be
constructed will not include those from age 0 to 4, among others.
This function returns the relevant age groups for a given cause of
mortality, such as that supplied to \texttt{yourcast()} via the
argument \texttt{depvar}.  You need to provide the function with this
cause of mortality via the parameter \texttt{disease}.  You may ignore
the argument \texttt{ebase}, which is for internal use only.  For
example, type the following at the command prompt to return the age
groups relevant for breast cancer, which is identified throughout the
WHO data files as ``brst'': 
\begin{verbatim}    
     > drop.ages(disease = ``brst'', age.groups = 5*0:16, ebase = 
     + env.base)
\end{verbatim}
The default values of the arguments \texttt{age.groups} and \texttt{ebase} 
are shown in this example.  Note that this function could be re-written to
broaden its applicability beyond forecasting mortality.  You need only
add your own dependent variables and relevant levels of the
grouped continuous index variable to the if/else cascade.

%%What does age.groups do?  It's not used in the function itself.

\subsection{Retrieving Names and Codes of the Geographic Nominal
Index Variable: \texttt{geographic.codes}}

The function \texttt{geographic.codes()} converts the names of levels
of the geographic nominal index variable into the corresponding
identification codes and vice versa, i.e.  from the identification
codes into the names.  In the case of forecasting mortality, of
course, these levels are countries.  The function can be called from
the global environment and requires you to supply as its input the
name of the file where the code---name associations are stored.  This
file is described in section \ref{sec:datafil}.  You must either
provide the full path and the name of the file via the argument
\texttt{filename} or rely upon the default file name and location by
setting \texttt{filename} equal to NULL.  The default location is the
sub-directory of your working directory provided to
\texttt{yourcast()} as argument \texttt{data.path} and the default
file name is that provided to \texttt{yourcast()} as argument
\texttt{codes.names}.  The argument \texttt{codes} is a boolean set
equal to TRUE by default.  The function returns a vector with length
equal to the number of levels of the geographic nominal index variable
in the file.  The values and the names of the vector elements are,
respectively, the names and the corresponding codes of the levels. For
example, type the following at the command prompt to retrieve the
names of the levels corresponding to the codes ``2450'' and ``4080'':
\begin{verbatim}
     > c.names <- geographic.codes(filename = NULL, codes = T)
     > c.names[``2450''] 
       ``USA'' 
     > c.names[``4080''] 
       ``France''
\end{verbatim}
Note that the codes must be entered as character strings, i.e.\ with
quotation marks.  If you want to find the codes given the names of the
levels, execute one of the following commands, which are case
insensitive when the argument \texttt{ignore.case} of the R-function
\texttt{grep} is equal to TRUE.
\begin{verbatim}
     > c.names[grep(``USA'', c.names, ignore.case = T)]
     > c.names[grep(``usa'', c.names, ignore.case = T)]
\end{verbatim}
Setting the argument \texttt{codes} equal to FALSE returns a vector
with the values and names of its elements equal to the numeric
identification codes and names of the corresponding levels,
respectively, the opposite of the vector returned when \texttt{codes}
is equal to TRUE.

\subsection{Changing the Format of the Output}

The function \texttt{yourcast()} produces lists of the observed and
predicted values for the dependent variable in two different formats.
In the pre-processed data file, the lists containing the dependent
variable data for the in- and out-of-sample periods have one element
for every cross-sectional unit comprised of the two-way
cross-classified levels of the geographic nominal index and the
grouped continuous index variables.  In the case of forecasting
mortality, this cross-sectional unit is a country--age group.  (See
the discussion in section \ref{sec:datastr} for more information.)
Such a list format is referred to as ``list.by.csid''.  Each element
of the list has as its name the combination of the identification
codes for the relevant levels of the geographic nominal index variable
and the grouped continuous index variable.  In the case of forecasting
mortality, these are the identification codes for the relevant country
and age group, such as ``245045'' for the USA and age group $45$.  The
elements consist of a one column matrix whose rows span the time
period, which are years in the case of forecasting
mortality.
   
Conversely, in the forecast object returned by the simulation, the
lists containing the dependent variable data for the in- and
out-of-sample periods have one element for every level of the
geographic nominal index variable.  In the case of forecasting
mortality, the elements of the list correspond to the countries
included in the analysis.  Such a list format is referred to as
``list.by.cntry''.  Each element of the list is named with the
identification code that corresponds to the level of the geographic
nominal index variable.  In the case of forecasting mortality, these
are the numeric country codes such as $2450$ for the USA.  The
elements of the lists consist of matrices whose columns are the levels
of the grouped continuous variable included in the analysis and rows
that span the time period.  In the case of forecasting mortality,
columns are the included age groups and rows are years.  This format is
more convenient for producing graphical output.

We provide functions that convert the first format into the second 
and vice versa:  the helper functions \texttt{list.by.csid()} and 
\texttt{list.by.cntry()}, which are described below. 
 
\subsubsection{list.by.cntry}
The function \texttt{list.by.cntry} takes as its input a list in the
first format (``list.by.csid'') and returns a list in the second
format (``list.by.cntry'').  The output is thus a list of $c$
matrices, one for each of the $c$ levels of the geographic nominal
index variable.  Each matrix has dimension $t$-by-$a$, where $t$ is
the length of the time series and $a$ is the number of levels of the
grouped continuous index variable.  Hence, a column of the matrix is
the time series for that level of the grouped continuous and
geographic nominal index variables.  The function has one input
parameter, \texttt{whoinsampy}, which is any list with the first,
``list.by.csid'' format, such as ``insampy'' or ``outsampy'' from the
preprocessed data file.  The other parameter, \texttt{ebase}, is a
dummy parameter for internal use that you may ignore.  Usage is as
follows:
\begin{verbatim}    
     > insampy <- list.by.cntry(whoinsampy = insampy, ebase = 
     + env.base)
\end{verbatim}

\subsubsection{list.by.csid}
Alternatively, the function \texttt{list.by.csid} takes as its input a
list in the second format (``list.by.cntry'') and returns a list in
the first format (``list.by.csid'').  The output is thus a list of
$c \times a$ matrices, one for each of the two-way cross-classified
cross-sectional (e.g., country--age group) units.  Each matrix has
dimension $t$-by-1, where $t$ is again the length of the time series.
The function has one input parameter, \texttt{insampy}, which is any
list with the second, ``list.by.cntry'' format, such as ``insampy'' or
``yhatin'' from the forecast object returned by \texttt{yourcast()}.
You should ignore all other parameters, since they are for internal
use only.  Usage is as follows:
\begin{verbatim}     
     > whoinsampy <- list.by.csid (insampy = insampy, col.name =  
     + ``dth'', ebase = env.base)
\end{verbatim}

%\subsubsection{bind.list}
%
%Given two lists of equal length, whose elements are matrices,
%it returns one list of same length, whose elements are the concatenation
%of the corresponding elements in the two lists.
%
%The inputs are two lists x and y of length n, 
%each element of x  and y must have 
%either the same number of rows or the same number of columns; 
%The logical bycol, which if TRUE concatenation is performed along columns
%otherwise along rows; the logical namex, if TRUE the resulting 
%list inherits the names of x otherwise it inherits the names of y
%The out put is the binding of the two lists x and y according to the 
%specification bycol.
%
%\begin{verbatim}
%x.bind.y <- bind.list(x,y,bycol=FALSE,namex=TRUE)
%\end{verbatim}
% 
%HMS:  This function seems pretty obscure.  Elena agrees to not
%release it independently, since it doesn't yield new insights into
%the data or code.

\subsection{Finding Covariates for Each Cross-Sectional Unit: \\
\texttt{list.covariates}}

The function \texttt{list.covariates()} finds the names of every
covariate that contributes to each of the cross-sectional units of
the observed and predicted dependent variable time series, after
collinearities are eliminated and relevant levels of the grouped
continuous variable are selected.  These cross-sectional units are
country--age groups in the case of forecasting mortality.  The
function takes as its input one parameter, \texttt{list}, which may be any
of the lists of covariate data stored in the preprocessed data file
such as ``insampx'', ``outsampx'', and ``covlist''; in addition, it may
also take as its input the variable ``coeff'' returned by
\texttt{yourcast()} as part of the forecast object.  The output of
\texttt{list.covariates()} is a list whose elements are matrices containing
the names of the covariates relevant to each cross-sectional unit.    
Usage is as follows:
\begin{verbatim} 
     > covs <- list.covariates(list = insampx)
     > covs <- list.covariates(list = res <- yourcast()$coeff)
\end{verbatim}%$

\subsection{Expanding Covariate Data: \texttt{expand.cov}}

Data input files for covariates of type ``age.independent'' contain a
single entry for each combination of the geographic nominal and time
index variables (e.g., in the case of forecasting mortality, for each
country--year unit) since the covariate is independent of the grouped
continuous index variable.  See section \ref{sec:datastr} for more
information.  Function \texttt{expand.cov()} builds the covariate
matrices to be used in the simulation from these input data files.
Specifically, it expands the number of entries by repeating each entry
as many times as there are levels of the grouped continuous index
variable, as specified via the parameter \texttt{age.groups}.
Expansion occurs by default for all levels of the geographic nominal
index variable, although you can expand the file for only a select set
of levels by setting the argument \texttt{select} equal to a string
vector, each element of which corresponds to the name of a level to be
expanded.  You supply the name of the covariate to expand using the
parameter \texttt{covname}, e.g.\ ``gdp'', the data file for which has
the name \texttt{``covname''.txt}, e.g. ``gdp.txt''.  The data file's
location in the directory path is communicated by the parameter
\texttt{covstring}.  You tell the function whether or not to save the
expanded data files in the same directory path under the name
\texttt{FULL\_``covname''.txt} using the boolean parameter
\texttt{save.FULL}, where ``cov'' is the value of the parameter
\texttt{covname}, e.g.\ ``FULL\_gdp.txt''.  Finally, the parameter
\texttt{ncol} tells \texttt{expand.cov} the number of columns in the
original reduced data file.  The function returns an expanded matrix
of three columns, where the first column contains the covariate
values, the second the CSTS identification codes, and the third either
the strata or ``NA'' (if the covariate is strata independent).  Usage
is as follows:
\begin{verbatim}
     > expand.cov(covname, covstring, ncol, type = NA, 
     + age.groups = 5*(0:16), select = NA, save.FULL = T)
\end{verbatim}

% Parameter select doesn't seem to allow you to select any set of
% countries:  code either does US only or all countries.  Also,
% do we want to rely on name of level (current code) as opposed to
% numeric code?  Former presumes existence of codes.names file.
% Also, what does parameter type do?  And is this expand.covariates
% in data functions?  There's no expand.cov that I can find.

\section{Programming Appendix}

%%  This entire section will need revision on the basis of changes
%%  made to the code; I have tried to anticipate as many as possible
%%  in what follows below.

This appendix sketches the implementation of \texttt{yourcast()},
describing some of the internal functions that perform key tasks.

\subsection{Preliminaries}
Your inputs provided in a file are processed by a call to the function
\texttt{setWHO()}, which updates the default globals as
necessary.  Validity checks on the resulting globals are performed by
a call to the function \texttt{setWHO()}.

%%  Perhaps describe the use of environments here briefly?

\subsection{Data Processing}
Data processing is performed by a call to the function
\texttt{make.mortality.data()}.  Function \texttt{yourcast()} calls 
\texttt{make.mortality.data()} when
either the argument \texttt{reuse.data} is equal to FALSE or
\texttt{reuse.data} is equal to TRUE but its call to the function
\texttt{reuse.data} does not result in a successful load of a
preprocessed data file.  
The function \texttt{make.mortality.data()} in turn calls
the helper functions \texttt{make.dth()} and
\texttt{make.covariates()}, which process the dependent and independent
input data files, respectively.

%%  This section will probably need revision.  I've effectively
%%  preserved the structure of the various functions, which may
%%  no longer be tenable.  For example, make.depvar may be eliminated
%%  now that it doesn't have to combine death and population data files
%%  to generate the mortality dep var.

Given covariate data files in the format described above, the function
\texttt{make.covariates()} returns a list with two elements.  The first
element is a matrix of size n-by-m, where each row contains the CSTS
unit data (for the user-selected level of the non- geographic nominal
index variable, if one exists) and each column is a covariate that 
you have selected.  The second element is a n-by-1 matrix containing the
CSTS unit code.

Given dependent variable data files in the format described above, the
function \texttt{make.dth()} returns a list with two elements, each a
n-by-1 matrix with the rows containing the CSTS unit data (for the
user-selected level of the non-geographic nominal index variable, if
one exists).  The first element of the list contains the value of the
dependent variable and the second contains the CSTS unit code.

The function \texttt{make.mortality.data()} combines the elements of these two
lists in one matrix.  It includes additional covariates if necessary
by adding columns: the constant, a column of 1s (if you  included
``cnst'' in the argument \texttt{cov}); time, a column containing the
code for the time of the CSTS unit (if you included ``time'' in
the argument \texttt{cov}); columns containing power transformations
of covariates and logged covariates (if you included power
transformations in the argument \texttt{cov}); and the lagged
dependent variable, a column containing the appropriately transformed
lag of the dependent variable, or any of the dependent variable data files,  
for the CSTS unit (if you included
``depvar'' in the argument \texttt{cov}, with transformation specified
in the argument \texttt{transform}).  Note that lower-order terms of
power transformation are added, each in its own column, if you 
did not supply them to the argument \texttt{cov}.  It also adds
columns containing the level of the geographic nominal index variable
and the code for the time period for each CSTS unit.  (Note that if
you selected ``time'' as a covariate, this latter column
duplicates the data in the ``time'' covariate column).

The function then transforms this large matrix.  Covariates that are
not power transformations are logged if you have specified logging 
by appending the prefix ``ln'' to the name of the covariate in
\texttt{cov}.  All CSTS units that do not correspond to the levels of
the geographic nominal and grouped continuous index variables
that you specified for the analysis (via the arguments
\texttt{usercntrylist}, \texttt{userages}, and \texttt{skip}) are
discarded.  In the latter case, a call is made to the function
\texttt{count.observations} to accomplish this.  0.5 is added to the
dependent variable for each observation to eliminate values of zero.
The user-specified transformation (via the argument
\texttt{transform}) is then applied to the dependent variable.
The matrix is expanded to encompass the out-of-sample period
by a call to the function \texttt{extend.datamat}.  Specifically, this
involves adding rows whose entries are ``NA''s except for the CSTS unit
code and which correspond to the cross-sectional units for the time
periods between the end of the in-sample time period, the argument
\texttt{yrest}, and the end of the out-of-sample period, the argument
\texttt{fore}.

This transformed matrix is then used to generate sub-matrices, one for
each cross-sectional unit, with rows corresponding to time and columns
to covariates. The individual cross- sectional unit matrices are then
transformed.  Matrices that are completely empty are deleted.
Completely missing covariates (over the entire time series) are
eliminated from each matrix by calls to the functions
\texttt{elim.all.na()} and \texttt{del.cov()}.  Covariates are lagged in
each matrix (in the course of which rows corresponding to times
preceding the first time for which the dependent variable is observed
are deleted; see the discussion under the argument \texttt{lag} for
further information regarding constraints on the lag structure) by
calls to the functions \texttt{lagMax()} and \texttt{lagcov()}.
Covariates completely missing in the in-sample period are deleted from
each matrix by calls to the functions \texttt{elim.all.na()} and
\texttt{del.cov()}.  Covariates with more than a given percentage of
in-sample missing values (specified via the argument
\texttt{lag.cutoff()} are deleted from each cross-sectional unit matrix
by calls to the functions \texttt{elim.cov()} and \texttt{del.cov()}.
User-specified covariates (via the argument \texttt{cov.select}) are
removed for user-specified cross-sectional units (for specific levels
of the grouped continuous variable selected via the argument
\texttt{age.select}) by calls to the functions \texttt{age.remove()} and
\texttt{del.cov()}.  The covariates are standardized for each
matrix if you so requested (via the argument
\texttt{standardize}).

The function then produces various processed data objects, e.g. lists
of in- and out-sample data (the result of sub-setting each matrix in
the list discussed above).  collinearities in each in-sample
cross-sectional unit covariate matrix are eliminated with a call to 
\texttt{preproc.elim.collinear()}, if you requested
elimination via the argument \texttt{elim.collinear}.
%%  Argument tol to yourcast is not currently used by preproc.elim.collinear; the value
%%  is instead hard-wired into the function, allowing no user control
%%  over the elimination of collinear covariates.  The global tol
%%  should be used instead.
The function \texttt{save.data.files()} is called to save these
objects and some globals to the user-specified path (via the argument
\texttt{out.path}) in a file.  If no forecasting is to be performed
(i.e. if argument \texttt{model} is equal to NA), \texttt{yourcast()}
returns after completing this save.  No objects are returned with the
function.

\subsection{Forecasting}
Forecasting is performed by a call to the method-specific function,
one of \texttt{ols()}, \texttt{lc()}, \texttt{glm.poisson()}, \texttt{cxc()},
or \texttt{gibbs.sampler()}.  Method-specific details are discussed below.

\texttt{Yourcast} implements the Lee-Carter forecasting method by a
call to the function \texttt{lc()}.  The function \texttt{lc()} in turn
calls the function \texttt{list.by.csid()} to re-organize the list
\texttt{yhatin}, the data on the dependent variable for the in-sample
period, into cross-sectional units defined solely by the geographic
nominal index variable.  It then calls the function \texttt{lc.model()}
to estimate the model parameters ($\gamma$, $\beta$, and $\alpha$) and
to calculate the predicted values of the dependent variable for the
in- and out-of-sample periods for each level of the geographic nominal
index variable.  Note that missing values for the in- sample $\gamma$s
are replaced by the predicted values before forecasting the
out-of-sample $\gamma$s using the random walk with drift model.
Missing values for the in-sample $\gamma$s are replaced with Na's
before being returned, however.  It also calls the function
\texttt{list.by.csid()} to re-organize the predicted values by the
standard cross-sectional units.

%%  (1)  Is there a mistake in the calculation of the covariance
%%  matrix of the data in function lc.model?  It's currently calculated by 
%%  mbar'mbar, where mbar is the matrix of centered data; doesn't this crossprod 
%%  matrix need to be multiplied by 1/(T-1) to obtain the covariance matrix?  
%%  (2)   The function may not be returning gamma. The assignment 
%%  coeff <- list(gamma=gamma, beta=beta.c) seems to need to use gamma.c instead 
%%  of gamma (there is no gamma that I see).

The Poisson forecasting method is implemented by a call to the
function \texttt{glm.poisson()}.  The function \texttt{glm.poisson()} in
turn calls the function \texttt{glm.colinear.poiss()}, which estimates
the model and eliminates collinear coefficients, if necessary, by a
call to the function \texttt{covdel()}.  collinearities greater than
\texttt{poisson.tol}, a parameter controlling the tolerance for linear
dependencies that is initially set at 0.99, are eliminated
successively (\texttt{poisson.tol} being decremented by
\texttt{delta.tol}, which is initially set to 0.005 on the basis of
experience) until either the collinear covariates are eliminated or
the decremented \texttt{poisson.tol} hits a floor of zero.
%%  The name of the argument tol should be changed--here I have suggested
%%  to poisson.tol--to eliminate confusion with the user-controlled argument 
%%  to yourcast, tol.  The value of delta.tol is currently set in glm.poiss
%%  to 0.005 (with a note that this is done on the basis of experience), but 
%%  the value passed to glm.collinear.poiss is 0, which will *not*
%%  eliminate collinearities.  
Any covariates eliminated in this manner are deleted from the relevant
globals (covlist, insampx, and outsampx). The function
\texttt{glm.poisson()} itself handles the initial setup, checks for
problems with degrees of freedom, makes any necessary changes to the
globals, and calculates the predicted values of the dependent variable
for the in- and out-of-sample periods.

The OLS forecasting method is implemented by a call to the function
\texttt{ols}.  The function \texttt{ols()} in turn calls the function
\texttt{svd.inv()} to calculate the inverse of the crossproduct of the
covariate matrix by singular value decomposition (using the
user-supplied tolerance \texttt{svdtol}); estimates the model; and
calculates the predicted values of the dependent variable for the
in-and out-of-sample periods by a call to the function
\texttt{make.forecast()}.
 
%%  (1)  There may be a mistake in the calculation of the standard deviation:
%%  n-1 instead of n-k degrees of freedom are used.  (Alternatively,
%%  it could be convenience driving this calculation.)
%%  (2)  The value svdtol is currently set in the svd.inv function definition;
%%  instead, the user-supplied parameter svdtol should be used.
%%  (3)  I have eliminated the transformation to log mortality, per
%%  earlier comments by Federico.

The Girosi-King forecasting method is implemented by a call to either
the function \texttt{gibbs.sampler()} or the function \texttt{cxc()}, which  
estimate the coefficients of the model and other quantities of interest; 
calculate the predicted values
of the dependent variable for the in- and out-of-sample periods by a
call to the function \texttt{make.forecast()}; and add the mean profile
of the grouped continuous index variable to the predicted values if
the prior was not zero mean by calls to the functions
\texttt{load.mean.age.profile()} and \texttt{modify.age.profiles()}.

%%  I have eliminated the transformation of the actual and predicted
%%  values to the log scale.  The description of the implementation
%%  will have to be updated once the Gibbs sampling/MAP
%%  implementations have both been added to the function.

The functions \texttt{lc()}, \texttt{poisson()}, \texttt{ols()},
\texttt{cxc()}, and \texttt{gibbs.sampler()} called by \texttt{yourcast()} 
return the forecast object.  Additionally, \texttt{yourcast()} calls the
function \texttt{conversion.cntry.mat()} to re-organize the first four
elements of the forecast object in a manner more amenable to graphing.
For each list, \texttt{conversion.cntry.mat()} calls the function
\texttt{list.by.cntry()} to obtain t-by-a matrices for each level of the
geographic nominal index variable, where t is the length of the time
series for the cross-section and a is the number of levels of the
grouped continuous index variable.  If the argument
\texttt{save.output} is equal to TRUE, \texttt{yourcast()} calls the
function \texttt{build.file.output()} to save the output file.  As its
last act, \texttt{yourcast()} invisibly returns the modified forecast
object, a six-element list comprised of the four modified (organized
by level of the geographic nominal variable) lists plus the unmodified
(organized by cross-sectional unit) two lists.

\bibliographystyle{polisci}
\bibliography{gk,gkpubs}
\end{document} 
